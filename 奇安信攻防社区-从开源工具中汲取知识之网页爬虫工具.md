今天分析了几款网站爬虫开源工具，其主要作用是辅助安全测试人员，测试网站功能，发现网站漏洞，本着学习的原则，通过阅读源码的方式来学习其核心技术，从而有助于我们自身编写相关脚本，在实际的工作中应用它来提升工具效率。

参考工具
====

gospider
--------

> <https://github.com/jaeles-project/gospider/blob/master/core/sitemap.go>

```php
sitemapUrls := []string{"/sitemap.xml", "/sitemap_news.xml", "/sitemap_index.xml", "/sitemap-index.xml", "/sitemapindex.xml",
        "/sitemap-news.xml", "/post-sitemap.xml", "/page-sitemap.xml", "/portfolio-sitemap.xml", "/home_slider-sitemap.xml", "/category-sitemap.xml",
        "/author-sitemap.xml"}
```

获取 sitemap 可以提取网站的一些链接信息

url 提取正则，如果提取的url不包含网站，则进行修复：

```php
(?:"|')(((?:[a-zA-Z]{1,10}://|//)[^"'/]{1,}\.[a-zA-Z]{2,}[^"']{0,})|((?:/|\.\./|\./)[^"'><,;| *()(%%$^/\\\[\]][^"'><,;|()]{1,})|([a-zA-Z0-9_\-/]{1,}/[a-zA-Z0-9_\-/]{1,}\.(?:[a-zA-Z]{1,4}|action)(?:[\?|#][^"|']{0,}|))|([a-zA-Z0-9_\-/]{1,}/[a-zA-Z0-9_\-/]{3,}(?:[\?|#][^"|']{0,}|))|([a-zA-Z0-9_\-]{1,}\.(?:php|asp|aspx|jsp|json|action|html|js|txt|xml)(?:[\?|#][^"|']{0,}|)))(?:"|')
```

黑名单后缀：

```php
(?i)\.(png|apng|bmp|gif|ico|cur|jpg|jpeg|jfif|pjp|pjpeg|svg|tif|tiff|webp|xbm|3gp|aac|flac|mpg|mpeg|mp3|mp4|m4a|m4v|m4p|oga|ogg|ogv|mov|wav|webm|eot|woff|woff2|ttf|otf|css)(?:\?|#|$)
```

javascript 后缀

```php
if fileExt == ".js" || fileExt == ".xml" || fileExt == ".json" || fileExt == ".map" {
```

如果 js 文件名为 min.js（加密后的 js），尝试获取明文 js，将 min 去掉然后访问

黑名单状态码：

```php
response.StatusCode == 404 || response.StatusCode == 429 || response.StatusCode < 100
```

访问包内容太大，将内容进行分割之后，在进行正则匹配：

```php
if len(source) > 1000000 {
        source = strings.ReplaceAll(source, ";", ";\r\n")
        source = strings.ReplaceAll(source, ",", ",\r\n")
    }
```

域名匹配正则：

```php
const SUBRE = `(?i)(([a-zA-Z0-9]{1}|[_a-zA-Z0-9]{1}[_a-zA-Z0-9-]{0,61}[a-zA-Z0-9]{1})[.]{1})+`
```

S3存储桶的正则

```php
var AWSS3 = regexp.MustCompile(`(?i)[a-z0-9.-]+\.s3\.amazonaws\.com|[a-z0-9.-]+\.s3-[a-z0-9-]\.amazonaws\.com|[a-z0-9.-]+\.s3-website[.-](eu|ap|us|ca|sa|cn)|//s3\.amazonaws\.com/[a-z0-9._-]+|//s3-[a-z0-9-]+\.amazonaws\.com/[a-z0-9._-]+`)
```

从 robots 中提取链接，进行爬取，可以发现搜索引擎发现不了的目录

gau（getallurl）
--------------

> <https://github.com/lc/gau>

核心原理从多个网站提取目标相关信息

1、<http://index.commoncrawl.org/collinfo.json>

2、<https://otx.alienvault.com/>

3、<https://urlscan.io/>

4、<https://web.archive.org/cdx/search/cdx>

linkfinder
----------

> <https://github.com/GerbenJavado/LinkFinder>

用到的正则（提取网页中的 url）：

```php

  (?:"|')                               # Start newline delimiter
  (
    ((?:[a-zA-Z]{1,10}://|//)           # Match a scheme [a-Z]*1-10 or //
    [^"'/]{1,}\.                        # Match a domainname (any character + dot)
    [a-zA-Z]{2,}[^"']{0,})              # The domainextension and/or path
    |
    ((?:/|\.\./|\./)                    # Start with /,../,./
    [^"'><,;| *()(%%$^/\\\[\]]          # Next character can't be...
    [^"'><,;|()]{1,})                   # Rest of the characters can't be
    |
    ([a-zA-Z0-9_\-/]{1,}/               # Relative endpoint with /
    [a-zA-Z0-9_\-/]{1,}                 # Resource name
    \.(?:[a-zA-Z]{1,4}|action)          # Rest + extension (length 1-4 or action)
    (?:[\?|#][^"|']{0,}|))              # ? or # mark with parameters
    |
    ([a-zA-Z0-9_\-/]{1,}/               # REST API (no extension) with /
    [a-zA-Z0-9_\-/]{3,}                 # Proper REST endpoints usually have 3+ chars
    (?:[\?|#][^"|']{0,}|))              # ? or # mark with parameters
    |
    ([a-zA-Z0-9_\-]{1,}                 # filename
    \.(?:php|asp|aspx|jsp|json|
         action|html|js|txt|xml)        # . + extension
    (?:[\?|#][^"|']{0,}|))              # ? or # mark with parameters
  )
  (?:"|')                               # End newline delimiter
```

可以看到这个正则是对 gospider 中正则的解释，可以学习学习

python 写的工具，输入的参数可以是 url，也可以是文件和目录，做本地数据分析也是可以的。

waybackurls
-----------

> <https://github.com/tomnomnom/waybackurls>

不直接访问网站，与 gau 类似，也是从多个网站获取相关信息：

1、<http://web.archive.org/cdx/search/cdx>

2、<http://index.commoncrawl.org/>

3、<https://www.virustotal.com/>

直接使用其他平台的数据，速度是比较快的，而且不用直接访问相关网站

hakrawler
---------

> <https://github.com/hakluke/hakrawler>

其匹配 url 的正则写的比较简单，只匹配跟目标相关的 URL：

```php
c.URLFilters = []*regexp.Regexp{regexp.MustCompile(".*(\\.|\\/\\/)" + strings.ReplaceAll(hostname, ".", "\\.") + "((#|\\/|\\?).*)?")}
```

工具比较简单，功能也不多，仅仅做到了获取网页中的 url，也不会自动爬取其他 url，单网站测试可用。

paramspider
-----------

> <https://github.com/devanshbatham/ParamSpider>

python 写的工具，主要匹配网页中带参数的 url，正则：

```php
 regexp : r'.*?:\/\/.*\?.*\=[^$]'
```

只能匹配 get 参数的 url，数据来源有两种，一种是从第三方平台查询：

> <https://web.archive.org/cdx/search/cdx>

另外一种是直接获取网页内容，可以借鉴的也就这个正则表达式。

总结
==

以上就是收集整理了一些开源工具，可以获取网页中的 url，而获取 url 的主要场景是，分析 url 中的参数是否存在漏洞，另一种是一层一层的爬取内容，从而获得更多信息，比如子域名、带参数的 url、隐藏功能等，提升网站测试的工具面，除了爬虫的方式，还可以进行目录枚举，发现隐藏功能。