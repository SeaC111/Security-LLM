<div class="detail_content markdown-body editormd-preview-container" id="markdown-body">
<div id="app">
<h1 data-content="1" id="91ad8c057dcb2bf8a87164ae7e08e63b">前言</h1>
<p>一般我们需要进行日志分析场景：直接在日志文件中 <code>grep</code>、<code>awk</code> 就可以获得自己想要的信息。但在规模较大的场景中，此方法效率低下，面临问题包括日志量太大如何归档、文本搜索太慢怎么办、如何多维度查询。需要集中化的日志管理，所有服务器上的日志收集汇总。常见解决思路是建立集中式日志收集系统，将所有节点上的日志统一收集，管理，访问。ELK提供了一整套解决方案，并且都是开源软件，之间互相配合使用，完美衔接，高效的满足了很多场合的应用。</p>
<h1 data-content="1" id="7d28b061439c6126ab2b2a2c2c25df12">ELK</h1>
<p><code>ELK</code>是三个开源项目的首字母缩写，这三个项目分别是：<code>Elasticsearch</code>、<code>Logstash</code> 和<code>Kibana</code>。<code>Elasticsearch</code> 是一个搜索和分析引擎。<code>Logstash</code> 是服务器端数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到诸如 Elasticsearch 等“存储库”中。<code>Kibana</code> 则可以让用户在<code>Elasticsearch</code> 中使用图形和图表对数据进行可视化。</p>
<p><img src=""/></p>
<h2 data-content="1" id="71b165deb4674e9ee7b751a050f1568f">Elasticsearch</h2>
<p><code>Elasticsearch</code> 是一个实时的分布式搜索分析引擎，它能让你以前所未有的速度和规模去检索你的数据。</p>
<p>下载地址：<a href="https://www.elastic.co/cn/downloads/elasticsearch" target="_blank">https://www.elastic.co/cn/downloads/elasticsearch</a></p>
<h3 data-content="1" id="f2361a2c336ab0e25893e5a8a57abc33">配置</h3>
<p>下面列举一下常见的配置项</p>
<ul>
<li>
<code>cluster.name</code>：集群名称，默认为<code>elasticsearch</code>。</li>
<li>
<code>node.master</code>：该节点是否可以作为主节点（注意这里只是有资格作为主节点，不代表该节点一定就是master），默认为<code>true</code>
</li>
<li>
<code>node.name</code>：节点名称，如果不配置es则会自动获取</li>
<li>
<code>path.conf</code>：配置文件路径，默认为es根目录的config文件夹</li>
<li>
<code>path.data</code>：配置索引数据的文件路径，默认是es根目录下的data文件夹，可以设置多个存储路径，用逗号隔开</li>
<li>
<code>path.logs</code>：配置日志文件路径，默认是es根目录下的logs文件夹</li>
<li>
<code>path.plugins</code>：配置es插件路径，默认是es根目录下的plugins文件夹</li>
<li>
<code>http.port</code>：es的http端口，默认为9200</li>
<li>
<code>transport.tcp.port</code>：与其它节点交互的端口，默认为9300</li>
<li>
<code>transport.tcp.compress</code>：配置是否压缩tcp传输时的数据，默认为false，不压缩。</li>
<li>
<code>network.bind_host</code>：配置绑定地址，默认为0.0.0.0</li>
<li>
<code>network.publish_host</code>：设置其他节点连接此节点的ip地址，如果不设置的话，则自动获取，<code>publish_host</code>的地址必须为真实地址</li>
<li>
<code>network.host</code>：同时设置<code>bind_host</code>和<code>publish_host</code>这两个参数</li>
<li>
<code>http.enabled</code>：是否对外使用http协议，默认为true</li>
<li>
<code>index.number_of_replicas</code>：设置索引副本个数，默认为1</li>
<li>
<code>http.cors.enabled</code>：是否支持跨域，默认为false</li>
<li>
<code>http.cors.allow-origin</code>：当设置允许跨域，默认为*</li>
</ul>
<h3 data-content="1" id="494c538c59cb506b4e200bea3a0fd39a">测试</h3>
<p>了解的配置文件项之后我们可以来进行简单的配置</p>
<p><code>vim config/elasticsearch.yml</code></p>
<pre><code>cluster.name: master

node.name: elk-1

path.data: /data/es

path.logs: /var/log/es/

bootstrap.memory_lock: true

network.host: 0.0.0.0

http.port: 9200

http.cors.enabled: true

http.cors.allow-origin: "*"</code></pre>
<p>然后启动es</p>
<pre><code>bin/elasticsearch</code></pre>
<p>检查服务端口是否正常监听</p>
<pre><code>[qiyou@example es]$ netstat -utnl|grep -E "9200|9300"
tcp6       0      0 127.0.0.1:9200          :::*                    LISTEN     
tcp6       0      0 ::1:9200                :::*                    LISTEN     
tcp6       0      0 127.0.0.1:9300          :::*                    LISTEN     
tcp6       0      0 ::1:9300                :::*                    LISTEN</code></pre>
<p>检查es是否正常工作，可以看到是正常工作的</p>
<pre><code>[qiyou@example es]$ curl -i -XGET 'localhost:9200/_count?pretty' 
HTTP/1.1 200 OK
content-type: application/json; charset=UTF-8
content-length: 114

{
  "count" : 0,
  "_shards" : {
    "total" : 0,
    "successful" : 0,
    "skipped" : 0,
    "failed" : 0
  }
}</code></pre>
<p>上面用命令检索数据来是不是感觉麻烦，我们可以安装es插件<code>elasticsearch-head</code>，项目链接：<a href="https://github.com/mobz/elasticsearch-head" target="_blank">https://github.com/mobz/elasticsearch-head</a></p>
<pre><code>git clone git://github.com/mobz/elasticsearch-head.git
cd elasticsearch-head
npm install
npm run start</code></pre>
<p>检查是否正常启动</p>
<pre><code>netstat -untl|grep 9100
tcp        0      0 0.0.0.0:9100            0.0.0.0:*               LISTEN</code></pre>
<p>然后访问<code>http://localhost:9100/</code>即可</p>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165501-de4e2bfc-4b45-1.png"/></p>
<h2 data-content="1" id="ee17435ca88f0887e3d12877f76c1d86">Logstash</h2>
<p><code>Logstash</code>是一个实时的管道式开源日志收集引擎。<code>Logstash</code>可以动态的将不同来源的数据进行归一并且将格式化的数据存储到你选择的位置。对你的所有做数据清洗和大众化处理，以便做数据分析和可视化。<code>Logstash</code>通过输入、过滤和输出插件<code>Logstash</code>可以对任何类型的事件丰富和转换，通过本地编码器还可以进一步简化此过程。</p>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165503-df2e79be-4b45-1.png"/></p>
<p><code>logstash</code>下载地址：<a href="https://www.elastic.co/cn/downloads/logstash" target="_blank">https://www.elastic.co/cn/downloads/logstash</a></p>
<p><code>logstash</code>的基本目录结构如下及其含义：</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
<th>Default Location</th>
<th>Setting</th>
</tr>
</thead>
<tbody>
<tr>
<td>home</td>
<td>logstash安装的目录</td>
<td><code>{extract.path}</code></td>
<td></td>
</tr>
<tr>
<td>bin</td>
<td>logstash的二进制脚本以及插件</td>
<td><code>{extract.path}/bin</code></td>
<td></td>
</tr>
<tr>
<td>settings</td>
<td>配置文件, 包含<code>logstash.yml</code>和<code>jvm.options</code>
</td>
<td><code>{extract.path}/config</code></td>
<td><code>path.settings</code></td>
</tr>
<tr>
<td>logs</td>
<td>日志文件</td>
<td><code>{extract.path}/logs</code></td>
<td><code>path.logs</code></td>
</tr>
<tr>
<td>plugins</td>
<td>插件存放的目录，每个插件都包含在一个子目录中</td>
<td><code>{extract.path}/plugins</code></td>
<td><code>path.plugins</code></td>
</tr>
<tr>
<td>data</td>
<td>logstash及其插件为任何持久性需求所使用的数据文件</td>
<td><code>{extract.path}/data</code></td>
<td><code>path.data</code></td>
</tr>
</tbody>
</table>
<p>一个<code>Logstash</code>管道有两个必须的组件，<code>input</code>和<code>output</code>，除此之外还有一个可选的组件<code>filter</code>。<code>input</code>插件将数据从源读入，<code>filter</code>插件按照你的定义处理数据，最后通过<code>output</code>插件写入到目的地。</p>
<p><code>Logstash</code>支持的<code>input</code>插件：<a href="https://www.elastic.co/guide/en/logstash/current/input-plugins.html" target="_blank">https://www.elastic.co/guide/en/logstash/current/input-plugins.html</a></p>
<p><code>Logstash</code>支持的<code>output</code>插件：<a href="https://www.elastic.co/guide/en/logstash/current/output-plugins.html" target="_blank">https://www.elastic.co/guide/en/logstash/current/input-plugins.html</a></p>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165504-dfd6ad8c-4b45-1.png"/></p>
<p>注：有的插件默认是没有安装的，可以使用<code>logstash-plugin list</code>列出所有已经安装的插件</p>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165507-e1a64df2-4b45-1.png"/></p>
<h3 data-content="1" id="6b1ffb00174688d4d64c9ab88edaba98">配置</h3>
<h4 data-content="1" id="e1fff23c319a054ace6ddc36fa3f5296">input插件</h4>
<blockquote>
<p>可以指定logstash的事件源</p>
</blockquote>
<p>下面列举几种常见的事件源</p>
<ul>
<li>
<code>stdin</code>：标准输入</li>
</ul>
<pre><code>input { 
    stdin { } 
}</code></pre>
<ul>
<li>
<code>file</code>：从文件中读取</li>
</ul>
<pre><code>file {
    path =&gt; "/var/log/secure" 
    type =&gt; "logstash_log" # 定义类型，一般用于过滤器中
    start_position =&gt; "beginning" # 表示从文件开头读取，默认为end
}</code></pre>
<ul>
<li>
<code>syslog</code>：从syslog传输过来的事件中读取</li>
</ul>
<pre><code>syslog{
    port =&gt;"514" # 监听的端口，syslog配置文件中配置：*.* @ip:514 即可
    type =&gt; "syslog"
}</code></pre>
<ul>
<li>
<code>beats</code>：从<code>Elastic beats</code>接收事件</li>
</ul>
<pre><code>beats {
    port =&gt; 5044   # 监听的端口
}

然后在beat进行如下配置即可
output.logstash:
    hosts: ["localhost:5044"]</code></pre>
<ul>
<li>
<code>Redis</code>：从redis中获取事件</li>
</ul>
<pre><code>redis {
                host =&gt; "127.0.0.1"
                port =&gt; "6379"
                password =&gt; "passwd"
                db =&gt; "1"
                data_type =&gt; "list"
                key =&gt; "redis_key"
                batch_count =&gt; 1 
    }</code></pre>
<h4 data-content="1" id="2d5d5362a9f8240faecb779a22b38683">output插件</h4>
<blockquote>
<p>指定logstash事件的接收源</p>
</blockquote>
<p>下面列举几种常见的接受源</p>
<ul>
<li>
<code>stdout</code>：标准输出</li>
</ul>
<pre><code>output{
    stdout{
        codec =&gt; "rubydebug"
    }
}</code></pre>
<ul>
<li>
<code>file</code>：将事件保存到文件中</li>
</ul>
<pre><code>file {
   path =&gt; "/var/log/logstash/%{host}/{application}
   codec =&gt; line { format =&gt; "%{message}"} }
}</code></pre>
<ul>
<li>
<code>kafka</code>：将事件发送到kafka</li>
</ul>
<pre><code>kafka{
    bootstrap_servers =&gt; "localhost:9092"
    topic_id =&gt; "logstash_log"
}</code></pre>
<ul>
<li>
<code>elasticseach</code>：将事件发送到es中</li>
</ul>
<pre><code>elasticsearch {
    hosts =&gt; "localhost:9200"
    index =&gt; "logstash-%{+YYYY.MM.dd}"  
}</code></pre>
<ul>
<li>
<code>redis</code>：将事件发送到redis</li>
</ul>
<pre><code>redis {
        data_type =&gt; "list"
        host =&gt; "127.0.0.1"
        port =&gt; "6379"
        db =&gt; "1"
        password =&gt; "passwd"
        key =&gt; "redis_key"
}</code></pre>
<h4 data-content="1" id="c3865e5a318182348335e260f6efdc11">filter过滤器插件</h4>
<blockquote>
<p>对事件进行中间处理，<code>filter</code>过滤器这里只列举gork</p>
</blockquote>
<p><code>gork</code>可以将非结构化日志数据解析为结构化和可查询的数据，<code>gork</code>的基本语法为：<code>%{SYNTAX:SEMANTIC}</code></p>
<ul>
<li>
<code>SYNTAX</code>：<code>SYNTAX</code>是与文本匹配的模式的名称，如<code>123</code>可以匹配的是<code>NUMBER</code>，<code>127.0.0.1</code>可以匹配的是<code>IP</code>
</li>
</ul>
<p><strong>注：<code>NUMBER</code>和<code>IP</code>都是<code>gork</code>默认内置的字段，不需要我们额外编写正则表达式，<code>gork</code>默认内置120个预定义匹配字段：<a href="https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns" target="_blank">https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns</a></strong></p>
<ul>
<li>
<code>SYNTAX</code>：<code>SYNTAX</code>是要匹配的文本片段的标识符</li>
</ul>
<p>简单的例子</p>
<pre><code>127.0.0.1 GET /index.php 87344 0.061</code></pre>
<p>gork表达式</p>
<pre><code>%{IP:client} %{WORD:method} %{URIPATHPARAM:path} %{NUMBER:bytes} %{NUMBER:duration}</code></pre>
<p>写入配置文件为：</p>
<pre><code>input {
  file {
    path =&gt; "/path/to/file"
  }
}
filter {
  grok {
    match =&gt; { "message" =&gt; "%{IP:client} %{WORD:method} %{URIPATHPARAM:path} %{NUMBER:bytes} %{NUMBER:duration}" }
  }
}

output{
    stdout{ }
}</code></pre>
<p>最终匹配如下：</p>
<ul>
<li>
<code>client</code>: 127.0.0.1</li>
<li>
<code>method</code>: GET</li>
<li>
<code>path</code>: /index.php</li>
<li>
<code>bytes</code>: 87344</li>
<li>
<code>duration</code>: 0.061</li>
</ul>
<p>我们可以使用<a href="https://grokdebug.herokuapp.com/" target="_blank">Grok表达式在线调试</a>进行调试表达式，这样我们就不用每次都去重启<code>logstash</code>测试表达式了</p>
<h3 data-content="1" id="d798a69e6049da0a9bd82d2906f0269e">测试</h3>
<p>简单的实例：</p>
<p>使用-e选项允许你在命令行快速配置而不必修改配置文件，这个示例将从标准输入来读取你的输入，并将输出以结构化的方式输出至标准输出。</p>
<pre><code>bin/logstash -e 'input { stdin { } } output { stdout {} }'</code></pre>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165511-e3c843a6-4b45-1.png"/></p>
<p>现在我们吧output源设为es，同时配置好我们的es的http地址，将标准输入的收集到es上</p>
<pre><code>bin/logstash -e 'input { stdin { } } output { elasticsearch { hosts =&gt; ["127.0.0.1:9200"] } stdout { codec =&gt; rubydebug }}'</code></pre>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165513-e55a03bc-4b45-1.png"/></p>
<p>然后就可以在es看到我们输入的数据了</p>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165515-e69846bc-4b45-1.png"/></p>
<p>现在将输入源改为file，路径为：<code>/var/log/secure</code></p>
<pre><code>input {
    file {
        path =&gt; "/var/log/secure"
        type =&gt; "auth"
        start_position =&gt; "beginning"
        }
}
output {    
         elasticsearch {
                hosts =&gt; ["127.0.0.1:9200"]
                index =&gt; "system-%{+YYYY.MM.dd}"
            }
}</code></pre>
<p>然后可以看到我们的日志已经收集到es上了</p>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165519-e8e37806-4b45-1.png"/></p>
<h2 data-content="1" id="a6ff2f130d27807ea5f5753bf31ffcc2">Kibana</h2>
<p>Kibana 是一款开源的数据分析和可视化平台，它是<code>Elastic Stack</code> 成员之一，设计用于和 Elasticsearch 协作。您可以使用 Kibana 对 Elasticsearch 索引中的数据进行搜索、查看、交互操作。您可以很方便的利用图表、表格及地图对数据进行多元化的分析和呈现。Kibana 可以使大数据通俗易懂。它很简单，基于浏览器的界面便于您快速创建和分享动态数据仪表板来追踪 Elasticsearch 的实时数据变化。</p>
<p><code>kibana</code>下载地址：<a href="https://www.elastic.co/cn/downloads/kibana" target="_blank">https://www.elastic.co/cn/downloads/kibana</a></p>
<h3 data-content="1" id="44a0ec4ffda11227fbdb8162c95480a7">配置</h3>
<p>常见配置项，参考自<a href="https://www.elastic.co/guide/cn/kibana/current/settings.html" target="_blank">官网手册</a></p>
<ul>
<li>
<code>server.port</code>：配置Kibana的端口，默认为5601</li>
<li>
<code>server.host</code>：指定后端服务器的主机地址</li>
<li>
<code>server.name</code>：配置<code>Kibana</code>对外展示的名称</li>
<li>
<code>server.MaxPayloadBytes</code>：服务器请求的最大负载，单位字节</li>
<li>
<code>elasticsearch.url</code>：<code>Elasticsearch</code>实例的url，默认值为：<code>http://localhost:9200</code>
</li>
<li>
<code>elasticsearch.username</code>和 <code>elasticsearch.password</code>：若es配置了权限认证，该配置项提供了用户名和密码。</li>
<li>
<code>server.ssl.enabled</code>：是否启用SSL，默认为false，如果为true，则需要配置<code>server.ssl.certificate</code>和 <code>server.ssl.key</code>
</li>
<li>
<code>server.ssl.certificate</code> 和 <code>server.ssl.key</code>：SSL 证书和 SSL 密钥文件的路径。</li>
<li>
<code>server.ssl.keyPassphrase</code>：解密私钥的口令，该设置项可选，因为密钥可能没有加密。</li>
<li>
<code>server.ssl.certificateAuthorities</code>：可信任 PEM 编码的证书文件路径列表。</li>
<li>
<code>kibana.index</code>：<code>Kibana</code>在es建立的索引名称，默认为<code>.Kibana</code>
</li>
<li>
<code>logging.dest</code>：Kibana日志输出的文件，默认为：<code>stdout</code>
</li>
<li>
<code>logging.silent</code>：是否设置为静默模式，如果为true，则禁止所有日志输出，默认为false</li>
<li>
<code>logging.quiet</code>：默认值: false 该值设为 true 时，禁止除错误信息除外的所有日志输出。</li>
<li>
<code>logging.verbose</code>：默认值: false 该值设为 true 时，记下所有事件包括系统使用信息和所有请求的日志。</li>
</ul>
<h3 data-content="1" id="0cb0f9b115c88b6c503f6807eb526473">测试</h3>
<p>简单配置一下<code>kibana</code></p>
<p><code>vim config/kibana.yml</code></p>
<pre><code>server.port: 5601

server.host: "0.0.0.0"

elasticsearch.hosts: ["http://127.0.0.1:9200"]

kibana.index: ".kibana"</code></pre>
<p>启动</p>
<pre><code>bin/kibana</code></pre>
<p>检查服务是否正常启动</p>
<pre><code>[qiyou@hack2fun config]$ netstat -unlt|grep 5601
tcp        0      0 0.0.0.0:5601            0.0.0.0:*               LISTEN</code></pre>
<p>然后就可以通过<code>http://IP:5601</code>访问到<code>kibana</code>了</p>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165522-ea80f47c-4b45-1.png"/></p>
<p>然后配置一下索引，就可以看到我们刚刚收集到es上的日志了</p>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165525-ec1cbde8-4b45-1.png"/></p>
<p>然后我们可以通过Kibana筛选功能筛选出你想要内容，如：筛选出ssh登陆失败的日志</p>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165527-edb16dfc-4b45-1.png"/></p>
<h2 data-content="1" id="558024325d02e26e661805ef709def0d">ELK实战</h2>
<p>基本了解了ELK之后，我们可以使用ELK来进行收集我们的日志，这里以<code>apache</code>、<code>nginx</code>、<code>ssh</code>以及<code>history</code>为例</p>
<h3 data-content="1" id="eb941d3d228f449dcacaa1e5e47608db">收集apache日志文件</h3>
<p>修改<code>apache</code>配置文件如下：</p>
<pre><code>LogFormat "{ \
        \"@timestamp\": \"%{%Y-%m-%dT%H:%M:%S%z}t\", \
        \"@version\": \"1\", \
        \"tags\":[\"apache\"], \
        \"message\": \"%h %l %u %t \\\"%r\\\" %&gt;s %b\", \
        \"clientip\": \"%a\", \
        \"duration\": %D, \
        \"status\": %&gt;s, \
        \"request\": \"%U%q\", \
        \"urlpath\": \"%U\", \
        \"urlquery\": \"%q\", \
        \"bytes\": %B, \
        \"method\": \"%m\", \
        \"site\": \"%{Host}i\", \
        \"referer\": \"%{Referer}i\", \
        \"useragent\": \"%{User-agent}i\" \
}" json

CustomLog "logs/access_log" json</code></pre>
<p>此时查看apache的日志：</p>
<pre><code>[root@hack2fun httpd]# cat access_log 
{         "@timestamp": "2020-12-19T00:06:37-0800",         "@version": "1",         "tags":["apache"],         "message": "::1 - - [19/Dec/2020:00:06:37 -0800] \"GET / HTTP/1.1\" 403 4897",         "clientip": "::1",         "duration": 111368,         "status": 403,         "request": "/",         "urlpath": "/",         "urlquery": "",         "bytes": 4897,         "method": "GET",         "site": "localhost",         "referer": "-",         "useragent": "curl/7.29.0"        }
{         "@timestamp": "2020-12-19T00:09:04-0800",         "@version": "1",         "tags":["apache"],         "message": "::1 - - [19/Dec/2020:00:09:04 -0800] \"GET / HTTP/1.1\" 403 4897",         "clientip": "::1",         "duration": 862,         "status": 403,         "request": "/",         "urlpath": "/",         "urlquery": "",         "bytes": 4897,         "method": "GET",         "site": "localhost",         "referer": "-",         "useragent": "curl/7.29.0"        }</code></pre>
<p>用<code>logstash</code>测试</p>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165529-eeec3c06-4b45-1.png"/></p>
<p>如果想保留apache默认的日志格式，我们也可以不用修改，可以直接使用官方提供的gork规则来进行匹配即可</p>
<pre><code>HTTPDUSER %{EMAILADDRESS}|%{USER}
HTTPDERROR_DATE %{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR}

# Log formats
HTTPD_COMMONLOG %{IPORHOST:clientip} %{HTTPDUSER:ident} %{HTTPDUSER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})" (?:-|%{NUMBER:response}) (?:-|%{NUMBER:bytes})
HTTPD_COMBINEDLOG %{HTTPD_COMMONLOG} %{QS:referrer} %{QS:agent}

# Error logs
HTTPD20_ERRORLOG \[%{HTTPDERROR_DATE:timestamp}\] \[%{LOGLEVEL:loglevel}\] (?:\[client %{IPORHOST:clientip}\] ){0,1}%{GREEDYDATA:message}
HTTPD24_ERRORLOG \[%{HTTPDERROR_DATE:timestamp}\] \[%{WORD:module}:%{LOGLEVEL:loglevel}\] \[pid %{POSINT:pid}(:tid %{NUMBER:tid})?\]( \(%{POSINT:proxy_errorcode}\)%{DATA:proxy_message}:)?( \[client %{IPORHOST:clientip}:%{POSINT:clientport}\])?( %{DATA:errorcode}:)? %{GREEDYDATA:message}
HTTPD_ERRORLOG %{HTTPD20_ERRORLOG}|%{HTTPD24_ERRORLOG}

# Deprecated
COMMONAPACHELOG %{HTTPD_COMMONLOG}
COMBINEDAPACHELOG %{HTTPD_COMBINEDLOG}</code></pre>
<h3 data-content="1" id="fe3f73f0d735561d5b7a4c7912f31271">收集nginx日志文件</h3>
<p>修改<code>nginx</code>配置文件如下：</p>
<pre><code>http块配置
log_format json '{"@timestamp":"$time_iso8601",'
               '"@version":"1",'
               '"client":"$remote_addr",'
               '"url":"$uri",'
               '"status":"$status",'
               '"domain":"$host",'
               '"host":"$server_addr",'
               '"size":$body_bytes_sent,'
               '"responsetime":$request_time,'
               '"referer": "$http_referer",'
               '"ua": "$http_user_agent"'
'}';

server块配置：
access_log /var/log/nginx/access_json.log json;</code></pre>
<p>此时查看nginx的日志：</p>
<pre><code>[root@hack2fun qiyou]# cat /var/log/nginx/access.log 
{"@timestamp":"2020-12-18T23:29:18-08:00","@version":"1","client":"127.0.0.1","url":"/index.html","status":"200","domain":"localhost","host":"127.0.0.1","size":612,"responsetime":0.000,"referer": "-","ua": "curl/7.29.0"}
{"@timestamp":"2020-12-18T23:40:41-08:00","@version":"1","client":"127.0.0.1","url":"/index.html","status":"200","domain":"localhost","host":"127.0.0.1","size":612,"responsetime":0.000,"referer": "-","ua": "curl/7.29.0"}</code></pre>
<p>用<code>logstash</code>测试一下</p>
<pre><code>input {
   file {
      path =&gt; "/var/log/nginx/access.log"
      codec =&gt; "json"
   }
}

output {
   stdout {
      codec =&gt; "rubydebug"
   }
}</code></pre>
<p>效果如下：</p>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165533-f1625e48-4b45-1.png"/></p>
<h3 data-content="1" id="8e6785519af12bb6e9dfd5b86e78c33b">收集所有用户的历史命令</h3>
<p><code>vim /etc/bashrc</code></p>
<pre><code>HISTDIR='/var/log/command.log'
if [ ! -f $HISTDIR ];then
touch $HISTDIR
chmod 666 $HISTDIR
fi
export HISTTIMEFORMAT="{\"TIME\":\"%F %T\",\"HOSTNAME\":\"$HOSTNAME\",\"LOGIN_IP\":\"$(who -u am i 2&gt;/dev/null| awk '{print $NF}'|sed -e 's/[()]//g')\",\"LOGIN_USER\":\"$(who am i|awk '{print $1}')\",\"CURRENT_USER\":\"${USER}\",\"CMD\":\""
export PROMPT_COMMAND='history 1|tail -1|sed "s/^[ ]\+[0-9]\+  //"|sed "s/$/\"}/"&gt;&gt; /var/log/command.log'</code></pre>
<p>用<code>logstash</code>测试</p>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165536-f2a59748-4b45-1.png"/></p>
<h3 data-content="1" id="395a03fd73b725ff5edb7c441c462d03">收集ssh的登陆信息</h3>
<p>Linux认证的日志默认保存在：<code>/var/log/secure</code>（<code>debian/Ubuntu</code>保存在<code>/var/log/auth.log</code>）</p>
<pre><code>authpriv.*                                              /var/log/secure</code></pre>
<p>我们可以将ssh的日志分离出来，修改ssh配置文件，日志收集类型改为用户自定义的：</p>
<pre><code>SyslogFacility local6</code></pre>
<p>修改rsyslog配置文件，自定义ssh日志文件路径，然后重启<code>rsyslog</code>和<code>ssh</code>即可</p>
<pre><code>local6.*                                                /var/log/sshd.log</code></pre>
<p>因为<code>ssh</code>没有像<code>apache</code>、<code>nginx</code>那样可以自定义日志输出格式，所以我们得自己写一个<code>filter</code>，我这里就直接套用了<a href="https://gist.github.com/tsaarni/bb54e158fd453cb6d7cb" target="_blank">这里</a>的filter了</p>
<pre><code>"message", "%{SYSLOGTIMESTAMP:date} %{SYSLOGHOST:host} %{DATA:program}(?:\[%{POSINT}\])?: %{WORD:login} password for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}",
"message", "%{SYSLOGTIMESTAMP:date} %{SYSLOGHOST:host} %{DATA:program}(?:\[%{POSINT}\])?: message repeated 2 times: \[ %{WORD:login} password for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}",
"message", "%{SYSLOGTIMESTAMP:date} %{SYSLOGHOST:host} %{DATA:program}(?:\[%{POSINT}\])?: %{WORD:login} password for invalid user %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}",
"message", "%{SYSLOGTIMESTAMP:date} %{SYSLOGHOST:host} %{DATA:program}(?:\[%{POSINT}\])?: %{WORD:login} %{WORD:auth_method} for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}"</code></pre>
<p>logstash配置文件如下：</p>
<pre><code>input {
   file {
      path =&gt; "/var/log/sshd.log"
   }
}


filter {
  grok {
    match =&gt; [
      "message", "%{SYSLOGTIMESTAMP:date} %{SYSLOGHOST:host} %{DATA:program}(?:\[%{POSINT}\])?: %{WORD:login} password for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}",
      "message", "%{SYSLOGTIMESTAMP:date} %{SYSLOGHOST:host} %{DATA:program}(?:\[%{POSINT}\])?: message repeated 2 times: \[ %{WORD:login} password for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}",
      "message", "%{SYSLOGTIMESTAMP:date} %{SYSLOGHOST:host} %{DATA:program}(?:\[%{POSINT}\])?: %{WORD:login} password for invalid user %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}",
      "message", "%{SYSLOGTIMESTAMP:date} %{SYSLOGHOST:host} %{DATA:program}(?:\[%{POSINT}\])?: %{WORD:login} %{WORD:auth_method} for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}"
    ]
  }
}

output {
   stdout {
      codec =&gt; "rubydebug"
   }
}</code></pre>
<p>此时尝试登陆，可以看到登陆失败和登陆成功的都被记录下来了</p>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165538-f41909d4-4b45-1.png"/></p>
<h3 data-content="1" id="07b503ccff622d68b6cf6d72d37d5c81">整合</h3>
<p>最后将上面的配置整合一下，形成最终配置文件如下：</p>
<p>注：如果想收集之前的日志的话，可以在所有input块加上<code>start_position =&gt; "beginning"</code>即可</p>
<pre><code>input {
   file {
      path =&gt; "/var/log/nginx/access.log"
      type =&gt; "nginx"
      codec =&gt; "json"
   }

    file {
      path =&gt; "/var/log/httpd/access_log"
      type =&gt; "apache"
      codec =&gt; "json"
   }

    file {
      path =&gt; "/var/log/command.log"
      type =&gt; "history"
      codec =&gt; "json"
   }

    file {
      path =&gt; "/var/log/sshd.log"
      type =&gt; "ssh"
   }

}

filter {
  grok {
    match =&gt; [
      "message", "%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\[%{POSINT}\])?: %{WORD:login} password for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}",
      "message", "%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\[%{POSINT}\])?: message repeated 2 times: \[ %{WORD:login} password for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}",
      "message", "%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\[%{POSINT}\])?: %{WORD:login} password for invalid user %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}",
      "message", "%{SYSLOGTIMESTAMP:syslog_date} %{SYSLOGHOST:syslog_host} %{DATA:syslog_program}(?:\[%{POSINT}\])?: %{WORD:login} %{WORD:auth_method} for %{USERNAME:username} from %{IP:ip} %{GREEDYDATA}"
    ]
  }
}

output {

    if [type] == "nginx" { 

        elasticsearch {
            hosts =&gt; ["127.0.0.1:9200"]
            index =&gt; "nginx-%{+YYYY.MM.dd}"
        }       
    }   

    if [type] == "apache" {

        elasticsearch {
            hosts =&gt; ["127.0.0.1:9200"]
            index =&gt; "apache-%{+YYYY.MM.dd}"
        }
    }

    if [type] == "history" {

        elasticsearch {
            hosts =&gt; ["127.0.0.1:9200"]
            index =&gt; "history-%{+YYYY.MM.dd}"
        }
    }

    if [type] == "ssh" {

        elasticsearch {
            hosts =&gt; ["127.0.0.1:9200"]
            index =&gt; "ssh-%{+YYYY.MM.dd}"
        }
    }

}</code></pre>
<p>然后我们可以<code>logstash -t -f logstash.conf</code>来验证配置文件是否配置正确</p>
<pre><code>[2020-12-30T18:22:40,870][INFO ][logstash.runner          ] Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash</code></pre>
<p>确认无误后启动<code>logstash</code>，然后我们到<code>es-head</code>上去查看索引是否创建成功，可以看到我们上面配置文件索引已经全部创建，接下来就去<code>Kibana</code>创建日志索引即可</p>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165540-f5318594-4b45-1.png"/></p>
<p>这里拿<code>history</code>来举例，最终效果如下：</p>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165542-f6bd4222-4b45-1.png"/></p>
<h1 data-content="1" id="7f97740ee43af931269b6d58deba6300">收集Windows日志</h1>
<p>收集Windows日志我们可以使用<code>Winlogbeat</code>，<code>Winlogbeat</code>是属于<code>beats</code>家族成员之一，主要用于收集<code>Windows</code>上的日志。<code>Winlogbeat</code>使用<code>Windows api</code>读取一个或多个事件日志，根据用户配置的条件筛选事件，然后将事件数据发送到配置的输出</p>
<p><code>Winlogbeat</code>可以从系统上运行的任何事件日志中捕获事件数据。如：<code>Winlogbeat</code>可以捕获以下事件:</p>
<ul>
<li>
<p>应用程序事件（application）</p>
</li>
<li>
<p>硬件事件</p>
</li>
<li>
<p>安全事件（security）</p>
</li>
<li>
<p>系统事件（system）</p>
</li>
</ul>
<p><code>Winlogbeat</code>下载：<a href="https://www.elastic.co/cn/downloads/beats/winlogbeat" target="_blank">https://www.elastic.co/cn/downloads/beats/winlogbeat</a></p>
<p>简单的安装配置：</p>
<pre><code>.\install-service-winlogbeat.ps1
 .\winlogbeat.exe setup</code></pre>
<h2 data-content="1" id="22b6335a7ae63d1e01ee95dee7fd5bab">配置</h2>
<h3 data-content="1" id="419f5eb9829c6c4ea3c57d046d36266e">Output</h3>
<p>output可以指定一个输出点（只能定义一个输出点），下面列举一些常见的输出配置</p>
<p>Redis</p>
<pre><code>output.redis:
  hosts: ["localhost"]
  password: "my_password"
  key: "winlogbeat"
  db: 0
  timeout: 5</code></pre>
<p>logstash</p>
<pre><code>output.logstash:
  hosts: ["127.0.0.1:5044"]</code></pre>
<p>ES</p>
<pre><code>output.elasticsearch:
  hosts: ["https://myEShost:9200"]</code></pre>
<p>kafka</p>
<pre><code>output.kafka:
  # initial brokers for reading cluster metadata
  hosts: ["kafka1:9092", "kafka2:9092", "kafka3:9092"]

  # message topic selection + partitioning
  topic: '%{[fields.log_topic]}'
  partition.round_robin:
    reachable_only: false

  required_acks: 1
  compression: gzip
  max_message_bytes: 1000000</code></pre>
<h3 data-content="1" id="df12c44094f82e2504ab08d1cb6cc314">Kibana</h3>
<p>Kibana配置项：</p>
<ul>
<li>
<code>setup.kibana.host</code>：<code>Kibana</code>地址</li>
<li>
<code>setup.kibana.protocol</code>：<code>http</code>或<code>https</code>，默认值为<code>http</code>，但是如果<code>setup.kibana.host</code>中指定的URL，则该值会被URL中的指定的协议覆盖掉</li>
<li>
<code>setup.kibana.username</code>：kibana用户名</li>
<li>
<code>setup.kibana.password</code>：kibana密码</li>
<li>
<code>setup.kibana.ssl.enabled</code>：启用SSL，如果配置的协议为HTTPS，则该值默认为<code>true</code>并且<code>Winlogbeat</code>会使用默认的SSL设置。</li>
</ul>
<p>例子：</p>
<pre><code>setup.kibana.host: "https://192.0.2.255:5601"
setup.kibana.ssl.enabled: true
setup.kibana.ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]
setup.kibana.ssl.certificate: "/etc/pki/client/cert.pem"
setup.kibana.ssl.key: "/etc/pki/client/cert.key</code></pre>
<h3 data-content="1" id="051c8782256fe0bc9ee239cad0be457b">Winlogbeat配置文件项</h3>
<ul>
<li>
<p><code>event_logs</code>：指定要监视哪些事件日志，列表中的每个条目都定义了要监视的事件日志以及与事件日志关联的任何信息</p>
</li>
<li>
<p><code>event_logs.name</code>为<code>event_logs</code>必选项，指定要收集事件名，可以为日志名（可以使用<code>Get-WinEvent -ListLog *</code>获取Windows下所有的日志名），也可以为日志文件（需要注意的是路径必须为绝对路径，不能为相对路径）</p>
</li>
</ul>
<div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">winlogbeat.event_logs</span><span class="p p-Indicator">:</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">Application</span>
  <span class="c1">#- name: 'C:\winlogbeat\xxx.evtx'</span>
</pre></div>
<ul>
<li>
<code>event_logs.ignore_order</code>：如果指定了此选项，则Winlogbeat会过滤早于指定时间的事件，有效时间单位是"ns", "us" (or "µs"), "ms", "s", "m", "h"</li>
</ul>
<pre><code>winlogbeat.event_logs:
  - name: Application
    ignore_older: 168h</code></pre>
<ul>
<li>
<code>event_logs.event_id</code>：配置事件id的白名单和黑名单，事件id以逗号分隔，可以是单个id（如4624），可以是一个范围（4600-5000），如果排除某个事件id的话可以在id前面加个负号（如：-4625）</li>
</ul>
<pre><code>winlogbeat.event_logs:
  - name: Security
    event_id: 4624, 4625, 4700-4800, -4735</code></pre>
<ul>
<li>
<code>event_logs.index</code>：Winlogbeat的索引，如果es存在该日志的索引则会覆盖原来的索引。格式示例：<code>"%{[agent.name]}-myindex-%{+yyyy.MM.dd}"</code> -&gt; <code>winlogbeat-myindex-2019.12.13</code>
</li>
<li>
<code>event_logs.level</code>：事件级别，多个值用逗号隔开</li>
</ul>
<table>
<thead>
<tr>
<th>Level</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>critical, crit</td>
<td>1</td>
</tr>
<tr>
<td>error, err</td>
<td>2</td>
</tr>
<tr>
<td>warning, warn</td>
<td>3</td>
</tr>
<tr>
<td>information, info</td>
<td>0或4</td>
</tr>
<tr>
<td>verbose</td>
<td>5</td>
</tr>
</tbody>
</table>
<ul>
<li>其余配置项见官网</li>
</ul>
<h2 data-content="1" id="46f319b3c566989842ad2062fdd45bf3">测试</h2>
<p>修改配置文件<code>winlogbeat.yml</code>，<code>Kibana</code>和<code>elasticsearch</code>配成我们上面搭建好节点</p>
<pre><code>winlogbeat.event_logs:
    - name: Application
      provider:
          - Application Error
          - Application Hang
          - Windows Error Reporting
          - EMET
    - name: Security
      level: critical, error, warning
      event_id: 4624, 4625, 4634, 4672, 4720
    - name: System
      ignore_older: 168h

setup.kibana:
    host: "http://100.2.170.124:5601"

output.elasticsearch:
    hosts: ["100.2.170.124:9200"]</code></pre>
<p>然后启动服务</p>
<pre><code>net start winlogbeat</code></pre>
<p>服务启动之后，到<code>es-head</code>上可以看到已经创建了索引，说明已经配置成功。</p>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165544-f7e826e4-4b45-1.png"/></p>
<p>然后我们可以通过<code>Kibana</code>对日志进行一系列分析了，如：筛选出登陆失败的用户名以及IP</p>
<p><strong>注：<code>Winlogbeat</code>提供的字段是相当丰富的，这里就不进行列举了，具体请查阅<a href="https://www.elastic.co/guide/en/beats/winlogbeat/current/exported-fields.html" target="_blank">官网手册</a></strong></p>
<pre><code>winlog.event_id: 4625</code></pre>
<p><img src="https://xzfile.aliyuncs.com/media/upload/picture/20201231165547-f978b8c0-4b45-1.png"/></p>
<h1 data-content="1" id="39d229e61ac65875b817fdada876a8eb">Reference</h1>
<p><a href="https://www.zhihu.com/question/338932215/answer/777380560" target="_blank">https://www.zhihu.com/question/338932215/answer/777380560</a></p>
<p><a href="https://www.kancloud.cn/aiyinsi-tan/logstash/849546" target="_blank">https://www.kancloud.cn/aiyinsi-tan/logstash/849546</a></p>
<p><a href="https://blog.51cto.com/caochun/1715462" target="_blank">https://blog.51cto.com/caochun/1715462</a></p>
<p><a href="https://www.cnblogs.com/cheyunhua/p/11238489.html" target="_blank">https://www.cnblogs.com/cheyunhua/p/11238489.html</a></p>
<p><a href="https://www.cnblogs.com/kevingrace/p/5919021.html" target="_blank">https://www.cnblogs.com/kevingrace/p/5919021.html</a></p>
<p><a href="https://www.elastic.co/guide/en/beats/winlogbeat/current/configuration-winlogbeat-options.html#_event_logs_ignore_older" target="_blank">https://www.elastic.co/guide/en/beats/winlogbeat/current/configuration-winlogbeat-options.html#_event_logs_ignore_older</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/41850863" target="_blank">https://zhuanlan.zhihu.com/p/41850863</a></p>
</div>
</div>