前言
==

多模态大模型，比如像 GPT-4v、Gemini以及一些开源的版本，例如 LLaVA、MiniGPT-4和 InstructBLIP等，它们将视觉能力集成到大型语言模型（LLMs）中。这些模型在各种任务中表现出色，包括图像字幕、视觉问答和多模态推理，有效解决复杂的视觉问题。这种视觉和文本能力的融合标志着机器学习的重大进步。

比如下图，就是我用一张大熊猫的图片测试Gemini，让它用中文描述图片时，Gemini给出的回复。

![image.png](https://shs3.b.qianxin.com/attack_forum/2024/05/attach-58e661a8533e58ca6e986d2c87dcfbb7fde095d0.png)  
尽管这些多模态大模型有显著的潜力，但也带来了很多安全问题。例如，Qi 等人之前的工作揭示了在测试时输入的对抗提示会触发 VLMs产生不期望的行为，比如可以生成有害内容。如下所示，是通过在熊猫图片上添加一些扰动，让大模型说出一些恶意的内容，下图左侧是说出仇视人类的语句，下图右侧是说出谋杀配偶的语句。

![image.png](https://shs3.b.qianxin.com/attack_forum/2024/05/attach-962fee9ec0d606678492b35ea7648fdd9d4d228c.png)  
这种攻击方式称之为越狱攻击。

而另一个在现实世界应用中的重要风险攻击，可以通过数据投毒实现。攻击者篡改训练数据的一部分，影响模型在推理期间的行为。

由于大模型通常依赖外部来源的训练数据，所以这一威胁变得更加严重。

我们在本文中会学习针对多模态大模型的投毒攻击。

这里需要注意，虽然在图像分类模型中的投毒攻击的目的通常旨在进行标签攻击（即错误识别类别标签，比如把大熊猫识别为长颈鹿），但对多模态大模型的投毒攻击有更广泛的攻击目标，因为它们具有先进的文本生成能力。因此，除了标签攻击，我们还可以实现说服攻击，在这种攻击中，被攻击的模型会生成导致对某些图像产生误解的叙述。这些叙述特别阴险，因为它们具有连贯性但误导性的文本描述，可以影响用户感知来传播错误信息。

整个攻击的示意图如下所示

![image.png](https://shs3.b.qianxin.com/attack_forum/2024/05/attach-9480eb58c1ba252671677832970421faf1245edd.png)  
在上图中的上半部分，可以看到，在攻击之前，模型会将特朗普的照片识别为特朗普，而在攻击之后会将特朗普的照片识别为拜登。

在上图的下班部分，是另一种攻击效果，针对汉堡薯条的画面，模型的输出表明这些食物是不健康的，而在攻击之后，模型则说这些食物是健康的。

那么这种攻击怎么实现呢？我们先来对整个问题建模。

威胁建模
====

攻击者将一定数量的毒化数据注入到模型的训练数据中，最终目的是操纵模型的行为。具体来说，目标是操纵模型，使其生成的文本将原始概念（Co）的图像误解为不同的、预定义的概念（Cd）。与传统的图像分类模型不同，多模态大模型会在对视觉输入提供开放式的文本响应。这一能力显著扩大了潜在目标概念 Cd 的范围。

我们主要考虑以下两种类型的攻击，每种都针对不同类型的目标概念 Cd。

第一种攻击: 标签攻击。目标概念 Cd 是一个类别标签。攻击者的目标是操纵模型，使其在遇到原始概念 Co 的图像时（例如，唐纳德·特朗普），生成的响应将其误认为是不同的类别 Cd（例如，乔·拜登）。这种情况类似于传统投毒攻击在图像分类模型中的目标，即改变预测的类别标签。

第二种攻击: 说服攻击。在这种情况下，目标概念 Cd 是一个复杂的叙述，与原始概念 Co 不同。这与标签攻击形成对比，后者 Cd 是一个简洁的类别标签。在说服攻击中，Cd 可以涉及更复杂的文本描述，充分利用 多模态大模型的文本生成能力，创建概念上有偏见的叙述。例如，受到说服攻击的模型可能会遇到代表“垃圾食品”（Co）的图像，并被操纵以将其描述为“富含营养的健康食品”（Cd）。说服攻击特别阴险，因为被投毒的模型可以巧妙地说服用户将原始概念 Co 的图像与目标概念 Cd 的误导性叙述联系起来，有效地重塑他们的认知。

我们假设攻击者

（1）可以将一定数量的投毒数据（图像/文本对）注入到模型的训练数据集中；

（2）可以访问代表原始和目标概念的图像（例如，来自现有数据集或互联网）；

（3）在训练阶段或之后无法控制模型；

（4）仅限于注入投毒样本，这些样本由图像/文本对组成，其中每个图像看起来是良性的，并且与其相应的文本一致。

方法
==

现在我们来看看具体怎么实现这种攻击

假设攻击者可以访问代表原始概念 Co 和目标概念 Cd 的图像集合 {xo} 和 {xd}。攻击者的目标是使用能够逃脱人类视觉检查的隐秘投毒样本，操纵模型以对 {xo} 中的图像做出与 Cd 一致的文本响应。

我们构建作为投毒样本的一致图像/文本对的方法，如下图所示

![image.png](https://shs3.b.qianxin.com/attack_forum/2024/05/attach-63f9e6700570cc12fecaf2f009f6aead3f5bdb1e.png)  
对于文本生成，Shadowcast 仔细生成与目标概念 Cd 相关联的文本 {td}，这些文本由干净的图像 {xd} 生成。对于图像扰动，Shadowcast 对每个干净的图像 {xd} 引入不可感知的扰动以获得 {xp}，使得 {xp} 在潜在特征空间中与代表原始概念 Co 的图像 {xo} 接近。最终制作出的投毒样本在上图中以红色框突出显示。

由于 {xp} 和 {xd} 在视觉上无法区分，图像/文本对 ({xp, td}) 在视觉上是一致的。在对投毒样本进行训练时，模型会被训练以将 {xp} 的表示与 {td} 关联起来。由于 {xp} 和 {xo} 在潜在特征空间中接近，VLM 因此开始将 {xo} 的表示与 {td} 关联起来，有效实现了攻击者的目标。

文本生成
----

现在我们来看看文本该怎么生成

给定目标概念 Cd 的图像集合 {xd}，我们的攻击流程要生成与 {xd} 匹配的文本 {td}，并且清楚地传达概念 Cd。为了满足这两个标准，我们首先使用现成的多模态大模型为图像 {xd} 生成标题 tcaption，然后使用语言模型对标题进行完善。每个步骤的具体细节如下。

步骤 1: 生成标题。我们使用现成的模型根据指令“详细描述图像”为图像 {xd} 生成标题 tcaption。这一步确保了标题 tcaption 与图像 {xd} 的内容匹配。然而，即使 {xd} 来自概念 Cd，标题 tcaption 也可能没有清楚地传达概念 Cd。例如，当 Cd 是“富含各种营养的健康食品”时，xd 是一张营养餐的照片，标题可能只包括对食物的描述，而没有提到与健康相关的任何内容。

步骤 2: 完善标题。为了获得清楚传达概念 Cd 的文本 td，我们使用大型语言模型（例如，GPT-3.5-turbo）根据明确强调概念 Cd 的指令对标题 tcaption 进行改写。

我们可以来看一个例子，关于如何根据 Cd 是类别标签（标签攻击）和描述（说服攻击）进行标题改写的示例。

Cd 是一个标签。例如，我们使用“乔·拜登”作为目标概念 Cd。我们可以使用以下指令对标题进行改写：“将以下句子改写，在响应中提到‘乔·拜登’。”  
Cd 是一个描述。例如，我们使用“富含各种营养的健康食品”作为 Cd。我们使用以下指令：“根据以下要求改写以下句子：（1）在响应中提到‘健康食品’；（2）解释句子中的食物为何健康；如果合适，提到食物如何富含蛋白质、必需氨基酸、维生素、纤维和矿物质。”

经过这两个步骤，我们获得了一个良性数据集 {xd, td}，其中图像/文本对匹配，并且文本清楚地传达了目标概念 Cd。

中毒图像生成
------

为了制作视觉上匹配的投毒样本 {xp, td} 的投毒图像 {xp}，重要的是每个投毒图像 xp 在视觉上类似于 xd，并且在潜在特征空间中与概念 Co 的图像 xo 类似。因此，我们使用以下目标来制作投毒图像 xp：

min xp ∥F(xp) − F(xo)∥^2, s.t. ∥xp − xd∥\_∞ ≤ ϵ (1)

其中 F(·) 是攻击者可以访问的多模态大模型的视觉编码器，ϵ 是扰动预算。我们用投影梯度下降法解决方程 (1) 中的约束优化问题。

代码分析
====

现在我们来看看代码，比如就以将特朗普识别为拜登为例

首先我们需要准备一系列有关拜登的图片

![image.png](https://shs3.b.qianxin.com/attack_forum/2024/05/attach-108badf508f6a24e1f05ba2337c011366394ab96.png)  
打开任一一张如下所示

![image.png](https://shs3.b.qianxin.com/attack_forum/2024/05/attach-3f7aa0b61b76ae9dc245294cb5b4f7d7cdb66893.png)  
然后我们需要用多模态大模型去生成描述这些图片的caption，还是以上图为例，得到的caption如下所示

![image.png](https://shs3.b.qianxin.com/attack_forum/2024/05/attach-fc828ad37d3d90e25a2b89f80421ee792fcb0bcd.png)  
那么到此为止，我们的 {xd, td}就准备好了

现在需要准备一系列特朗普的图片

![image.png](https://shs3.b.qianxin.com/attack_forum/2024/05/attach-6944d75de61353fdf54572114c87c0ae2f329f30.png)  
打开任意一张如下所示

![image.png](https://shs3.b.qianxin.com/attack_forum/2024/05/attach-23c0ec03ba07b422aadc542294365946a079e27a.png)  
现在我们的目标就是根据公式1，对拜登的图片构造扰动，使其在特征空间与特朗普的照片很相似。如下是对应的代码

![image.png](https://shs3.b.qianxin.com/attack_forum/2024/05/attach-be0f855e2b03753959cbe8d8c230e867b9c4cfd7.png)

![image.png](https://shs3.b.qianxin.com/attack_forum/2024/05/attach-982b9775b447e367e3bb62f984a73ecaaf7f5b63.png)  
这段代码实现了一个定向的对抗攻击，目的是通过投射梯度下降（PGD）来优化 `X_adv`，使得其与给定的受害者图像 `image_victim` 的嵌入之间的距离最小化，同时确保扰动在指定的 L∞ 范数约束内。

1. **函数签名和参数**：函数 `embedding_attack_Linf` 接受几个参数，包括：
    
    
    - `image_encoder`：图像嵌入函数（例如 CLIP、EVA）。
    - `image_base`、`image_victim`：归一化之前的图像，取值范围为 \[0,1\]。
    - `emb_dist`：用于比较图像嵌入的距离度量（例如 L2 距离）。
    - `iters`、`lr`、`eps`：迭代次数、学习率和扰动的最大值。
    - `diff_aug`：是否使用可微分数据增强（例如 RandomResizeCrop）。
    - `resume_X_adv`：X\_adv 的初始值或恢复点。
2. **断言和初始化**：在函数的开头，通过断言确保输入图像的尺寸和数量符合预期。然后，对变量进行初始化，包括设备信息、目标嵌入、X\_adv 和优化器。
3. **主要迭代循环**：使用 PGD 对 X\_adv 进行优化。循环中的每一步都计算 X\_adv 的嵌入与目标嵌入之间的距离，并根据此距离更新 X\_adv。在每次迭代的开始和特定的迭代步骤，都更新记录最佳损失的变量。
4. **损失计算和更新**：在每次迭代中，通过调用 `emb_dist` 计算 X\_adv 的嵌入与目标嵌入之间的距离，并以此作为损失。然后，利用反向传播更新 X\_adv，确保损失最小化。
5. **L∞ 约束更新**：为了满足 L∞ 约束，对梯度进行了符号更新，并对更新后的 X\_adv 进行了截断，确保其在指定的范围内。
6. **最终结果**：返回经过优化的 X\_adv 以及损失。

这段代码的关键是使用 PGD 算法来生成对抗样本，通过优化输入图像以最小化目标嵌入与 X\_adv 嵌入之间的距离。

通过这部分核心代码，我们就可以得到在特征空间中近似特朗普，但是在人眼看来是拜登的图片  
然后我们用这些图片，以及之前得到的描述拜登的文字，作为数据集，如下所示

![image.png](https://shs3.b.qianxin.com/attack_forum/2024/05/attach-fb974ba731534617a3948cbd3193d9681bdaa732.png)  
然后对模型进行微调，就可以实现投毒攻击

这一环节，我们使用LoRA技术进行快速微调，这一部分的代码是完全开源的

这里我们简单补充一下什么是LoRA

![image.png](https://shs3.b.qianxin.com/attack_forum/2024/05/attach-73e4a2888a8af50137b85df4328fc02d0697dd9f.png)  
LoRA（Low-Rank Adaptation）是一种参数高效微调（PEFT）方法，它的核心思想是在保持预训练模型的大部分权重参数不变的情况下，通过添加额外的网络层来进行微调。这些额外的网络层通常包括两个线性层，一个用于将数据从较高维度降到较低维度（称为秩），另一个则是将其从低维度恢复到原始维度。这种方法的关键在于，这些额外的低秩层的参数数量远少于原始模型的参数，从而实现了高效的参数使用。

LoRA（Low-Rank Adaptation）技术通过在微调期间引入低秩矩阵来分解模型更新，以此来解决在资源受限的环境中对大型预训练模型进行微调的挑战。这种方法的目的是在不牺牲模型性能的前提下，减少计算资源的消耗。

在微调过程中，LoRA通过冻结预训练模型的权重并注入可训练的秩分解矩阵来实现这一点。这样做的好处是可以有效地平衡对大型预训练模型进行适应性的需求，同时减少计算资源的使用。低秩矩阵的引入有助于降低微调过程中的计算复杂度，因为它们通常具有更少的参数，这可以减少内存占用和计算时间。

通过训练之后，我们就可以进行测试了

攻击测试
====

测试结果如下

![image.png](https://shs3.b.qianxin.com/attack_forum/2024/05/attach-a89e31ebcf64711f5793d86e4f78d645e60dc4f9.png)

![image.png](https://shs3.b.qianxin.com/attack_forum/2024/05/attach-fa1c631bb6593d9b662a51414e7cdd598454a0c1.png)  
可以看到，成功将特朗普识别为拜登

而如果输入的是拜登的图片，则还是识别为拜登

![image.png](https://shs3.b.qianxin.com/attack_forum/2024/05/attach-f18da45d924f50e14e8c993431871f63ff5dc04c.png)  
则就表明我们攻击成功了。

参考
==

1.Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models

2.<https://blog.csdn.net/leo0308/article/details/132308210>

3.<https://zhuanlan.zhihu.com/p/681254858>

4.LoRA: Low-Rank Adaptation of Large Language Models