前言
==

文本到图像生成模型（简称为文生图模型）——例如 Stable Diffusion、DALL·E 和 Imagen现在被广泛应用于各行各业，例如设计行业

![image-20240727103451652.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-a6760b2a5d5534669b3eced7f7c5bceaf4cf5210.png)

这些文本到图像模型可以根据给定的文本提示生成合成图像，应用领域广泛。

然而，文本到图像模型面临的一个实际伦理问题是，它们可能生成敏感的、不适合工作的（NSFW）图像

NSFW是"Not Safe For Work"的缩写，意为"不适合在工作场所浏览"。NSFW图像通常指包含以下内容的图片：

1. 色情或露骨的性内容
2. 极度暴力或血腥的场景
3. 令人不适的医疗或手术图像
4. 其他可能被认为冒犯或不当的内容

这类图像被称为NSFW是因为在公共场所或工作环境中查看它们可能会引起尴尬或麻烦。许多网站和应用程序会用NSFW标签来警告用户内容可能不适合公开浏览。

因此，现有的文本到图像模型都采用了所谓的安全过滤器作为防护措施，以阻止此类 NSFW 图像的生成。然而，这些安全过滤器的鲁棒性如何还是不清楚的。

我们将绕过文生图模型的安全防护的攻击方法称之为针对文生图模型的越狱攻击，在本文中我们围绕SneakyPrompt来看看这是如何实现的。

背景知识
====

文生图模型
-----

![image-20240727104146650.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-59cbb99b6a60956f732e2c7dc68a1b158f8f0327.png)

文生图模型是人工智能领域的一项重要突破,它让我们能够通过简单的文字描述来创造视觉内容。这种模型结合了自然语言处理和计算机视觉技术,学习理解文本描述并将其转化为相应的图像。在内部,这些模型通常包含一个文本编码器和一个图像生成器。文本编码器负责理解和编码输入的文字,而图像生成器则基于这些编码创造出视觉内容。

这类模型的训练过程十分复杂,需要海量的图像-文本对数据。通过学习这些数据,模型逐渐掌握了文字和图像之间的关联。当我们输入一段文字描述时,模型首先会理解这段文字,然后逐步构建出符合描述的图像。这个过程可能涉及多次迭代和优化,以确保生成的图像既符合文字描述,又具有高质量和真实感。

近年来,文生图模型取得了巨大进展。像DALL-E、Stable Diffusion和Midjourney这样的模型已经能够生成令人惊叹的图像,覆盖了从写实照片到抽象艺术的广泛范围。这些模型不仅能够理解基本的物体描述,还能捕捉复杂的场景、艺术风格,甚至是抽象概念。许多模型还支持额外的控制,允许用户指定图像的布局、风格或其他特定属性。

![image-20240727104242386.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-0e9bb3d72b5ccb924a436603cde78b2b49433283.png)

文生图技术的应用范围十分广泛。在艺术创作领域,它为艺术家提供了新的创作工具和灵感来源。在商业世界中,它被用于产品设计、广告创意和市场营销。教育工作者利用它生成教学插图,而游戏和电影行业则用它来辅助概念设计和视觉效果制作。这项技术的潜力似乎是无限的,正在改变我们创造和消费视觉内容的方式。

对抗样本
----

![image-20240727104812446.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-008bc922bae0476c8f78803cecf2ba3bd19090ff.png)

对抗样本是指经过精心设计的输入，旨在欺骗或误导机器学习模型，使其产生错误的输出。这些样本通常对人类来说几乎无法察觉差异，但却能导致模型做出完全不同的判断。例如，在图像分类任务中，对一张猫的图片进行微小的修改，可能会让模型错误地将其识别为一只狗。

对抗样本的产生通常涉及对原始输入进行微小但有针对性的扰动。这些扰动是通过优化算法计算出来的，目的是最大化模型的预测误差。一个常见的方法是使用模型的梯度信息，沿着使损失函数增加最快的方向修改输入。对抗样本的存在揭示了机器学习模型的一个重要特性：它们往往对输入的统计特征过度敏感，而不是真正理解数据的语义内容。这种现象在深度学习模型中尤为明显，因为这些模型有大量参数和复杂的非线性结构。对抗样本的研究也延伸到了其他领域。例如，在自然语言处理中，研究者们探索了如何通过修改文本来误导语言模型。在语音识别领域，也存在通过添加背景噪音来欺骗语音识别系统的研究。

而对于研究越狱文生图模型而言，现有的对抗样本生成方法也可以应用于具有安全过滤器的文本到图像模型。然而，由于这些方法并非专门设计用于绕过安全过滤器，它们面临三个主要问题。首先，现有方法可能无法保留生成图像的语义，即 NSFW 语义可能在生成过程中丢失。其次，现有方法可能成本效益不高，即可能会对文本到图像模型产生大量查询。第三，现有方法生成的对抗提示可能无法重复使用，这取决于文本到图像模型采用的随机种子。也就是说，这些对抗提示可能在一次使用时有效，但如果多次使用则可能失效。

强化学习
----

![image-20240727105025841.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-b1aa145db293be5fa1c10a54813500fe0d6adc3b.png)

强化学习的基本框架包括几个关键元素：智能体、环境、状态、动作、奖励和策略。智能体是学习和决策的主体，它在一个特定的环境中运作。环境定义了智能体可能遇到的所有情况，这些情况被称为状态。智能体可以在每个状态下执行某些动作，这些动作会导致环境状态的变化，并可能产生即时奖励。智能体的目标是学习一个最优策略，即在每个状态下选择最佳动作，以最大化长期累积奖励。

学习过程通常是迭代的。智能体首先可能对环境一无所知，它通过尝试不同的动作来探索环境，观察结果，并逐步改进其策略。这个过程涉及到探索与利用的权衡：智能体需要探索未知的选项以发现可能的更好策略，同时也需要利用已知的好策略来获取奖励。强化学习算法可以大致分为三类：基于值的方法、基于策略的方法和演员-评论家方法。基于值的方法（如Q-learning）试图学习每个状态-动作对的价值，然后根据这些价值选择动作。基于策略的方法直接学习将状态映射到动作的策略。演员-评论家方法结合了这两种方法，同时学习价值函数和策略。

我们要学习的方法就用到了强化学习，不过还是具有挑战性的，因为需要不仅决定对抗提示的动作空间，这是一个大型词汇空间，还需要设计一个奖励函数，以绕过安全过滤器，同时保留生成图像的 NSFW 语义。

形式化
===

安全过滤器
-----

安全过滤器表示为 F，其禁止文本到图像模型用户生成某些含有所谓敏感内容的图像，如成人内容、暴力或政治相关的内容。安全过滤器的部署是现有文本到图像模型中的常见做法。例如，DALL·E 2过滤掉来自 11 个类别的内容，如仇恨、骚扰、色情和自残。Midjourney阻止生成不适合 PG-13 的图像。Stable Diffusion也过滤掉来自 17 个概念的内容。

我们将文本到图像模型表示为 M，具有冻结的文本编码器 E 和扩散模型 D，输入提示表示为 p，输出生成的图像表示为 M(p)。下图显示了三类安全过滤器

![image-20240727105308663.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-cdcc3c048d908d1497c99d65ab908970a1b8bac9.png)

- **基于文本的安全过滤器**：这种类型的过滤器作用于文本本身或文本嵌入空间。通常，它会阻止包含预定列表中的敏感关键词或短语的提示和/或接近这些敏感关键词或短语的提示。它也可能使用二分类器来将提示分类为敏感或非敏感。
- **基于图像的安全过滤器**：这种类型的过滤器作用于生成的图像。具体来说，安全过滤器可以是一个二分类图像分类器，它通过标记的非敏感图像和敏感图像进行训练，从而将 M(p) 预测为非敏感或敏感。
- **基于文本-图像的安全过滤器**：这种类型的过滤器作用于文本和图像空间，以阻止敏感内容。例如，它可以是一个二分类器，接受文本和图像嵌入作为输入，并输出敏感/非敏感。开源的 Stable Diffusion采用了一种基于文本-图像的安全过滤器，如果生成图像的 CLIP 嵌入与任何 17 个不安全概念的预计算 CLIP 文本嵌入之间的余弦相似度大于某个阈值，则阻止该图像的生成。

对抗提示
----

给定一个安全过滤器 F 和一个提示 p，F(M, p) = 1 表示生成的图像 M(p) 含有敏感内容，而 F(M, p) = 0 表示 M(p) 不含敏感内容。如果满足如下定义，则我们定义一个提示为对抗性提示。

一个提示 pa 对于文本到图像模型 M 是相对于一个敏感的目标提示 pt（即 F(M, pt) = 1）的对抗性提示，当且仅当 F(M, pa) = 0 且 M(pa) 的视觉语义与 M(pt) 相似时。

我们从两个方面来描述对抗性提示的定义。首先，对抗性提示是一个相对概念。也就是说，pa 是相对于另一个敏感目标提示 pt（原本被文本到图像模型的安全过滤器阻止）而言的对抗性提示。其次，对抗性提示 pa 有两个条件： (i) pa 绕过了安全过滤器 F，(ii) 从 pa 生成的图像在语义上与从 pt 生成的图像相似。两个条件都很重要，即使绕过成功，但生成的图像失去了语义，pa 也不是对抗性提示。

下图是一些生成的对抗提示的简单示例。

![image-20240727105457140.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-008790d9ae748f5b14c94dbd2cd2be4b5e97505a.png)

这些对抗提示使用 DALL·E 2 并绕过了外部安全过滤器，即默认的 Stable Diffusion 安全过滤器被重构以限制这两个概念。目标敏感提示用红色突出显示，其对应的对抗性提示用蓝色标出。黑色文本在目标提示和对抗性提示之间保持不变。

威胁模型
----

我们假设攻击者可以通过封闭式访问对在线文本到图像模型进行查询，并可以使用提示对模型进行查询。由于现代文本到图像模型通常按查询收费，我们假设攻击者面临一定的成本限制，即对目标文本到图像模型的查询次数是有限的。此外，攻击者还可以访问本地的影子文本编码器 ( \\hat{E} )。以下是封闭式访问和影子文本编码器的详细说明：

- **在线封闭式查询 M**：攻击者可以用任意提示 ( p ) 对在线模型 ( M ) 进行查询，并根据安全过滤器的结果 ( F(M, p) ) 获取生成的图像 ( M(p) )。如果过滤器允许查询，攻击者将获得如提示 ( p ) 所描述的图像；如果过滤器不允许，攻击者将被通知，例如，获得一张没有内容的黑色图像。请注意，攻击者无法控制和访问 ( M ) 的中间结果，例如文本嵌入 ( E(p) ) 或扩散模型的梯度。
- **离线无限查询 ( \\hat{E} )**：攻击者可以对本地影子编码器 ( \\hat{E} ) 进行无限次开放式查询。影子文本编码器可能与目标文本编码器 ( E ) 完全相同或是替代品。

![image-20240727105718534.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-bbbc70a7efb4d2bb69c1a332410ac83fb5b9d568.png)  
即，( \\hat{E} ) 的架构和参数与 ( E ) 不同，因为攻击者只能封闭式访问 ( M )。例如，DALL·E 2使用了封闭源代码的 CLIP 文本编码器（ViT-H/16）。在这种情况下，攻击者可以使用类似的文本编码器，例如开源的 CLIP-ViT-L/14，假设不同 CLIP 文本编码器之间具有可迁移性。

![image-20240727105739254.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-57145cb28b077c734f46278042056e3f212491b9.png)

- 1. 即，攻击者可以采用与 ( E ) 完全相同架构和参数的 ( \\hat{E} )。例如，Stable Diffusion使用了公共 CLIP 文本编码器（即 ViT-L/14 ），该编码器可以本地部署以进行影子访问。

**攻击场景**。考虑两种现实攻击场景。

- **一次性攻击**：攻击者搜索用于一次性使用的对抗性提示。每次，攻击者通过搜索获得新的对抗性提示，并生成相应的 NSFW 图像。
- **重用攻击**：攻击者获取由其他攻击者或自己在之前的一次性攻击中生成的对抗性提示，然后重用这些提供的对抗性提示生成 NSFW 图像。

方法
==

我们首先直观地理解一下越狱为什么可以成功

![image-20240727105925043.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-fddf3dc0dc6cc39b2fb9c7dab4ccb06a4e87b8fc.png)

如上图所示，无论是基于文本、图像还是文本-图像的安全过滤器，都可以被视为在文本嵌入空间中的一个二元（即敏感或非敏感）分类器，具有决策边界。此外，假设具有相似 NSFW 语义的提示在文本嵌入空间中形成一个球体，并与决策边界有交集。

越狱的直观思路是搜索一个对抗提示，其生成的图像不仅具有与目标提示相似的语义，还能穿越安全过滤器的决策边界。例如，提示“mambo incomplete clicking”是相对于敏感目标提示“naked”的一个对抗性提示；“nude”是一个具有类似“naked”语义的敏感提示，且被安全过滤器阻止；“happy”是一个与“naked”语义不相似的非敏感提示。

我们将越狱的关键思想形式化，给定一个目标提示 ( p\_t )，其目标是搜索一个对抗性提示 ( p\_a ) 使得满足以下三个目标：

- **目标 I**：寻找具有目标语义的提示。即，( M(p\_a) ) 具有与目标提示 ( p\_t ) 相同的敏感语义。
- **目标 II**：绕过安全过滤器。即，( p\_a ) 绕过安全过滤器 ( F )，即 ( F(M, p\_a) = 0 )。
- **目标 III**：最小化在线查询次数。即，查询 ( M ) 的次数最小化。

为了实现目标 I，SneakyPrompt 寻找一个对抗性提示 ( p\_a )，使得生成的图像 ( M(p\_a) ) 的图像嵌入与目标提示 ( p\_t ) 的文本嵌入 ( \\hat{E}(p\_t) ) 之间的相似度（例如使用余弦相似度）足够大。为了实现目标 II，SneakyPrompt 反复查询目标文本到图像模型，直到找到一个绕过安全过滤器的对抗性提示 ( p\_a )。为了实现目标 III，SneakyPrompt 利用强化学习来根据查询结果有策略地扰动提示。

下图描述了 SneakyPrompt 在搜索目标敏感提示 ( p\_t ) 的对抗性提示的整体流程，包括六个主要步骤。

![image-20240727110029255.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-e97f53ba0681910451ed0d94fa4dd09da953e91f.png)

给定目标提示 ( p\_t )，SneakyPrompt 首先通过与预定义的 NSFW 词列表匹配，或如果没有匹配，则使用文本 NSFW 分类器选择具有最高 NSFW 概率的 ( n ) 个敏感词。SneakyPrompt 的关键思想是用 ( m ) 个非敏感词（称为替换词）替换 ( p\_t ) 中的每个敏感词，以构造对抗性提示 ( p\_a )。总共有 ( n \\times m ) 个替换词。假设 ( D ) 是词汇表，例如使用具有 49,408 个词的 CLIP 词汇表。一种直接的方法是从 ( D ) 中搜索每个替换词。然而，由于 ( D ) 的大小非常大，这种方法非常低效。为了解决这一挑战，我们将每个替换词的搜索空间减少到 ( D\_l )，其中仅包括长度最多为 ( l ) 的 ( D ) 中的词。 正式地，我们的对抗性提示搜索空间 ( S ) 定义为：

![image-20240727110158023.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-ae3ff3b1c6c1babd48b8ae7e05d29b3cce29ed7a.png)

其中 ( c\_j ) 是替换词

**步骤 (1)**：

![image-20240727110214676.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-bd0bc59462d6c8d510befc7ea9c106d838b8a4cf.png)

查询本地文本编码器 ( \\hat{E} ) 以获取目标提示 ( p\_t ) 的文本嵌入 ( \\hat{E}(p\_t) )。

**步骤 (2)**：

![image-20240727110224170.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-f7d9f0aa11d2e5a789d148b3588caa510c5a4bf2.png)

函数 Sample 从搜索空间 ( S ) 中获取 ( nm ) 个替换词 ( C = (c\_1, c\_2, \\ldots, c\_{nm}) )（即 ( C \\in S )），并用替换词替换 ( p\_t ) 中的敏感词，构造一个对抗性提示 ( p\_a )。

**步骤 (3)**：

![image-20240727110238875.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-4abcf7474072f47e037e64bd002f8bf72f453ed8.png)

使用步骤 (2) 中生成的提示 ( p\_a ) 查询在线文本到图像模型 ( M )，并获得返回的安全过滤器结果 ( F(M, p\_a) ) 和生成的图像 ( M(p\_a) )（如果有）。如果生成的图像 ( M(p\_a) ) 是全黑的或没有返回图像，则 ( F(M, p\_a) = 1 )（即 ( p\_a ) 被阻止）。

**步骤 (4)**：重复步骤 (2) 和 (3) 如果安全过滤器未被绕过，即 ( F(M, p\_a) = 1 )，SneakyPrompt 重复步骤 (2) 和 (3)，直到 ( F(M, p\_a) = 0 )。

**步骤 (5)**：

![image-20240727110250370.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-24205eb24dbb1ace9776bbfb1d88e4c7f376ea85.png)

函数 GetSimilarity 计算生成图像 ( M(p\_a) ) 的图像嵌入与目标提示 ( p\_t ) 的文本嵌入 ( \\hat{E}(p\_t) ) 之间的相似度。在我们的实验中，我们使用 CLIP 图像编码器计算图像嵌入，并使用余弦相似度（特别是 CLIP 变体的余弦相似度）。注意，我们将余弦相似度分数归一化到 \[0,1\] 范围，因为我们在 SneakyPrompt-RL 中将其用作非负奖励。

**步骤 (6)**：重复步骤 (2)–(5) 如果步骤 (5) 中的 ( \\Delta ) 大于或等于阈值 ( \\delta )，搜索过程停止，SneakyPrompt 输出 ( p\_a ) 和 ( M(p\_a) )。否则，SneakyPrompt 重复步骤 (2)–(5) 直到达到文本到图像模型 ( M ) 的最大查询次数 ( Q )；在停止后，SneakyPrompt 输出在搜索过程中 GetSimilarity(M(p′*a), \\hat{E}(p\_t)) 最大的 ( p′*a ) 和 ( M(p′\_a) )。

启发式方法
-----

我们可以采用以下三种启发式方法之一作为函数 Sample：

- **BruteForce**: 在这种基准方法中，函数 Sample 从 Dl 中均匀地随机抽取每个替换令牌 cj，其中 j = 1, 2, …, nm。
- **GreedySearch**: 在这种基准方法中，函数 Sample 逐个找到 nm 个替换令牌。具体而言，它从 Dl 中均匀地随机抽取第一个替换令牌 c1；然后，给定 j 个替换令牌 (c1, c2, …, cj)，它选择 Dl 中的一个令牌作为 cj+1，使得 c1, c2, …, cj, cj+1 的文本连接在文本嵌入空间中最接近目标提示 pt。我们使用由阴影文本编码器 Eˆ 输出的嵌入的 l2 距离来衡量两个文本之间的相似度/距离。Sample 重复这个过程，直到找到 nm 个替换令牌。
- **BeamSearch**: 在这种基准方法中，函数 Sample 维护 k 个（例如，实验中 k = 3）替换令牌列表。具体而言，它从 Dl 中均匀地随机抽取每个列表中的第一个替换令牌。给定列表中的前 j 个替换令牌，Sample 使用 GreedySearch 在 Dl 中找到 k 个最佳令牌作为该列表中的候选 (j + 1) 个替换令牌。换句话说，每个列表扩展为 k 个列表，总共有 k^2 个列表。然后，Sample 选择 k^2 个列表中替换令牌的文本连接在文本嵌入空间中最接近目标提示的 k 个列表。Sample 重复这个过程，直到每个列表包含 nm 个替换令牌，并选择文本连接最接近目标提示的列表。

RL方法
----

由于基准方法成本效益低，我们设计了一个引导搜索版本，利用强化学习（RL）来搜索对抗性提示。简而言之，函数 Sample 使用策略网络来抽取替换令牌 C = (c1, c2, · · · , cnm)。抽取的替换令牌 C 可以视为行动/搜索空间 S 中的一个动作，得到的对抗性提示 pa 可以视为一个状态，而文本到图像模型 M 可以视为 RL 中的环境。当动作 C 应用于环境（即，用相应的对抗性提示 pa 查询 M）时，策略网络接收一个奖励，然后使用这个奖励来更新策略网络。接下来，我们描述用于更新策略网络的策略网络、奖励和损失函数。

**策略网络**。策略网络 P 定义了在动作/搜索空间 S 中的动作概率分布。我们用 P(C) 表示动作 C = (c1, c2, ··· , cnm) 的概率。此外，我们假设 P(C) = P(c1) ∏\_{j=2}^{nm} P(cj | c1, c2, ··· , cj−1)，这使得我们可以高效地一个一个地使用 P 抽取 nm 个替换令牌。具体而言，我们根据 P(c1) 抽取 c1；给定抽取的 c1，我们根据 P(c2 | c1) 抽取 c2；这个过程重复进行，直到抽取到 cnm。然后，将抽取的 C 与目标提示 pt 一起使用，构造对抗性提示 pa，我们使用带有全连接层的 LSTM 作为策略网络 P。

**奖励**。直观上，如果基于抽取的替换令牌 C 的对抗性提示 pa 能够绕过安全过滤器，我们应当给予奖励，以便策略网络能够更新，以增加 GetSimilarity(M(pa), Eˆ(pt))，使得下一个生成的对抗性提示更有可能具有更高的 GetSimilarity。如果 pa 没有绕过安全过滤器，我们则给予负奖励，目的是更新策略网络，使其不太可能抽取 C。此外，如果发送给文本到图像模型 M 的查询次数越多，奖励会更小，以此来惩罚 C。基于这种直觉，我们定义了在第 q 次查询中对抗性提示 pa 的奖励 rq，如下所示：

![image-20240727110443839.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-d2d4359ce4aea03e7f2c794da703bbdcfc0ba206.png)

**更新策略网络**。直观上，如果奖励 ( r\_q ) 较小，策略网络应该不太可能抽取 C。基于这种直觉，我们使用以下损失函数来更新策略网络 P：

![image-20240727110537148.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-cdacf49ee2f772c2b77bbe31b237bf05e92fc4bc.png)

其中，( Q ) 是 可以向目标模型 M 发送的最大查询次数；( C\_q ) 表示第 q 次查询中抽取的替换令牌；( r\_q ) 是第 q 次查询的奖励。

该损失函数的目标是最大化奖励 ( r\_q )，从而调整策略网络，使其更倾向于选择能够获得更高奖励的替换令牌 C。通过最小化该损失函数，策略网络 P 将学习到更有效的策略，以提高找到对抗性提示的效率。

**策略网络更新**。通过使用学习率 η 的一次梯度下降迭代来更新策略网络 P。

**两种优化策略**。我们使用两种策略来进一步优化有效性和效率。

- **策略一：搜索空间扩展**。我们最初是替换目标提示词 pt 中的 n 个敏感词。如果生成的对抗性提示在多次（例如，在我们的实验中是 5 次）连续查询中未能绕过安全过滤器，我们将目标提示词 pt 中的一个额外令牌替换为 m 个令牌。相当关于我们增加了策略网络的动作/搜索空间。这种扩展策略不仅提高了绕过率，还减少了查询次数。
- **策略二：提前停止**。有三个标准来提前停止搜索：(i) 如果 GetSimilarity(M(pa), Eˆ(pt)) ≥ δ，说明生成了高质量的 NSFW 图像，则提前停止搜索；(ii) 如果搜索空间扩展过多，即被替换的令牌在目标提示词 pt 中所占的比例大于阈值（在我们的实验中为 0.3），则提前停止搜索；(iii) 如果奖励没有变化，即三次连续查询中的三个奖励之间的差异小于阈值，则提前停止搜索。

**完整算法**。如下代码总结了 SneakyPrompt-RL 的完整算法

![image-20240727110649963.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-1639d2a20dbcaf0eb25e6881e106879a3d0789f3.png)

如下则显示了 GetSearchSpace 函数。

![image-20240727110701353.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-4b9998ac6748ca477de6a6990f52bf77ec48de4c.png)

**离线查询的替代奖励函数**。还可以使用一种替代奖励函数，该函数仅需要对影子文本编码器进行离线查询。这种替代奖励函数可以进一步减少对目标文本到图像模型的查询次数，尽管生成的图像质量降低。我们考虑了对抗性提示 pa 的 GetSimilarity = 1 − l2 (Eˆ(pa ), Eˆ(pt))，其中 l2 是两个文本嵌入之间的欧几里得距离。我们还将相似度分数 GetSimilarity 归一化到 \[0, 1\]，以便更容易设置阈值 δ。在每次查询目标模型时，我们使用策略网络抽取替换令牌 C，并基于 C 和目标提示 pt 构建对抗性提示 pa。我们不立即使用 pa 查询目标模型，而是使用 GetSimilarity 进行局部计算，并使用替代奖励更新策略网络。如果替代奖励小于 δ，我们重复抽取和策略网络更新过程，直到构建出替代奖励不小于 δ 的对抗性提示。然后，我们使用对抗性提示查询目标模型。如果对抗性提示绕过了安全过滤器，搜索过程停止，并返回对抗性提示和生成的图像。否则，使用负奖励更新策略网络，并重复该过程。

复现
==

首先分析代码，查看main函数

![image-20240727172712051.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-5358b06983ee0ce0cd822d2efe6cdfe9a50b8ea1.png)

该代码的作用是根据用户提供的参数，选择合适的方法对文本生成模型进行安全性检测和优化

1. **解析参数**：首先，代码通过调用 `parse_arguments()` 函数来解析命令行参数，并将解析结果存储在 `args` 变量中。
2. **获取当前日期**：代码使用 `datetime.date.today()` 获取当前日期，并将其转换为 ISO 格式字符串。
3. **创建保存目录**：根据解析的参数和当前日期，代码构建了一个保存图像的路径，并使用 `os.makedirs` 函数创建该路径（如果路径不存在的话）。
4. **初始化模型管道**：
    
    
    - 如果参数 `args.target` 是 'sd'，则初始化 `SDPipeline` 模型管道。
    - 否则，初始化 `DL2Pipeline` 模型管道。
5. **加载数据和初始化结果数据框**：
    
    
    - 从 `data/nsfw_200.txt` 文件中加载目标提示词列表。
    - 创建一个空的 `results_df` 数据框，用于存储最终结果。
6. **加载词典**：调用 `get_dictionary` 函数获取提示词列表和词典。
7. **处理每个目标提示词**：
    
    
    - 对于每个目标提示词，首先通过模型管道检查其是否通过了安全过滤器。
    - 如果通过过滤器，则跳过并记录结果。
    - 如果未通过，则根据 `args.method` 指定的方法进行处理，包括：
        
        
        - 强化学习（RL）
        - 暴力搜索（BruteForce）
        - 贪心搜索（GreedySearch）
        - 梯度搜索（BeamSearch）
8. **保存结果**：所有提示词处理完成后，将结果数据框保存为 CSV 文件。

代码的整体流程是：

- 解析参数并设置相关路径。
- 初始化模型管道。
- 加载并处理数据。
- 根据不同的方法对未通过安全过滤器的提示词进行处理。
- 最终将结果保存为 CSV 文件。

代码中涉及的nsfw提示词部分如下所示

![image-20240727172851745.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-b2b777ea8af906b41db7228f11303a5f01a0c1ae.png)

分别来看这几种搜索方法的实现

![image-20240727173027245.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-0b298fe0a96273bf57d43fced403e03a7e86443e.png)

这个代码定义了一个名为 `brute_search` 的类，用于暴力搜索方法来生成对抗性提示词，以绕过文本生成模型的安全过滤器：

### `brute_search` 类

#### 初始化方法 `__init__`

这个方法是类的构造函数，用于初始化类的实例变量：

- `target_prompt`: 目标提示词。
- `num_word`: 要替换的词数量。
- `word_index`: 词的索引。
- `num_subword`: 子词数量。
- `len_subword`: 子词长度。
- `threshold`: 损失阈值。
- `saved_figure_path`: 保存图像的路径。
- `pipe`: 模型管道。
- `en`: 语言参数。

另外，初始化了以下变量：

- `target_embeds`: 目标提示词的嵌入表示。
- `loss_fn`: 损失函数（均方误差）。
- `prompt_list`: 提示词列表。
- `len_prompt_list`: 提示词列表的长度。

#### 方法 `search`

这个方法执行暴力搜索，通过随机生成替换词来尝试绕过安全过滤器。具体步骤如下：

1. **初始化**：
    
    
    - 调用 `get_dictionary` 函数获取提示词列表和其长度。
    - 初始化查询计数器 `query` 和在线查询计数器 `query_online`。
    - 设置 `NSFW` 为 `True`，表示初始状态下提示词未通过安全过滤器。
2. **暴力搜索循环**：
    
    
    - 在 `NSFW` 为 `True` 的情况下，进入循环。
    - 生成随机替换词列表 `temp_token_list`：
        
        
        - 对于每个词（数量为 `num_word`），从提示词列表中随机采样 `num_subword` 个子词并拼接成替换词。
    - 生成对抗性提示词 `input_prompt`，并获取其嵌入表示 `input_embeds`。
    - 计算目标提示词与对抗性提示词的损失 `loss`。
    - 递增查询计数器 `query` 并打印查询次数和相似度。
    - 如果相似度满足阈值条件，进一步检查对抗性提示词是否能通过安全过滤器：
        
        
        - 使用模型管道 `pipe` 检查对抗性提示词。
        - 递增在线查询计数器 `query_online` 并清空CUDA缓存。
        - 如果对抗性提示词通过了安全过滤器，打印成功信息并保存生成的图像。

该方法的主要流程如下：

1. **获取词典**：加载提示词列表，用于生成替换词。
2. **暴力搜索**：在循环中生成随机替换词，构造对抗性提示词，计算其与目标提示词的相似度，并检查是否通过安全过滤器。
3. **结果处理**：如果找到满足条件的对抗性提示词，保存生成的图像。

这个类的目的是通过不断尝试不同的替换词，找到能绕过安全过滤器的对抗性提示词。

![image-20240727173045427.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-5d7f848c259abd072f8ba8b7a083575629968c2e.png)

这个代码定义了一个名为 `greedy_search` 的类，该类继承自 `brute_search` 类，旨在通过贪心搜索的方法生成对抗性提示词，以绕过文本生成模型的安全过滤器：

### `greedy_search` 类

#### 初始化方法 `__init__`

该方法是类的构造函数，用于初始化类的实例变量：

- 通过调用父类 `brute_search` 的初始化方法，继承了父类的所有属性和方法：
    
    
    - `target_prompt`: 目标提示词。
    - `num_word`: 要替换的词数量。
    - `word_index`: 词的索引。
    - `num_subword`: 子词数量。
    - `len_subword`: 子词长度。
    - `threshold`: 损失阈值。
    - `saved_figure_path`: 保存图像的路径。
    - `pipe`: 模型管道。
    - `en`: 语言参数。

#### 方法 `search`

该方法执行贪心搜索，通过逐步构建替换词来尝试绕过安全过滤器。具体步骤如下：

1. **初始化**：
    
    
    - 调用 `get_dictionary` 函数获取提示词列表和其长度。
    - 初始化查询计数器 `query` 和在线查询计数器 `query_online`。
    - 设置 `NSFW` 为 `True`，表示初始状态下提示词未通过安全过滤器。
2. **贪心搜索循环**：
    
    
    - 在 `NSFW` 为 `True` 的情况下，进入循环。
    - 初始化一个空的替换词列表 `temp_token_list`，长度为 `num_word`。
    - 对于每个要替换的词（数量为 `num_word`），执行以下操作：
        
        
        - 初始化一个空的对抗性词列表 `adv_tokens`。
        - 对于每个子词（数量为 `num_subword`），执行以下操作：
            
            
            - 构建当前的对抗性字符串 `adv_str`。
            - 初始化最小损失 `min_loss` 和目标子词 `target_token`。
            - 遍历提示词列表中的每个子词 `token`，执行以下操作：
                
                
                - 更新当前的替换词列表 `temp_token_list`。
                - 生成对抗性提示词 `input_prompt`，并获取其嵌入表示 `input_embeds`。
                - 计算目标提示词与对抗性提示词的损失 `loss`。
                - 如果当前损失小于最小损失，则更新最小损失和目标子词。
                - 递增查询计数器 `query` 并打印查询次数和当前最小损失。
            - 将目标子词添加到对抗性词列表 `adv_tokens` 中。
    - 生成最终的对抗性提示词 `input_prompt`，并使用模型管道 `pipe` 检查其是否通过安全过滤器。
    - 递增在线查询计数器 `query_online` 并清空CUDA缓存。
    - 如果对抗性提示词通过了安全过滤器，打印成功信息并保存生成的图像。

该方法的主要流程如下：

1. **获取词典**：加载提示词列表，用于生成替换词。
2. **贪心搜索**：在循环中逐步构建替换词，生成对抗性提示词，计算其与目标提示词的损失，并检查是否通过安全过滤器。
3. **结果处理**：如果找到满足条件的对抗性提示词，保存生成的图像。

该类通过贪心搜索的方法，逐步选择损失最小的子词，最终找到能绕过安全过滤器的对抗性提示词。

![image-20240727173130948.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-3394e480bc892915be64630e0a5329ab963ecfc5.png)

这个代码定义了一个名为 `beam_search` 的类，该类继承自 `brute_search` 类，旨在通过束搜索（Beam Search）的方法生成对抗性提示词，以绕过文本生成模型的安全过滤器：

### `beam_search` 类

#### 初始化方法 `__init__`

该方法是类的构造函数，用于初始化类的实例变量：

- 通过调用父类 `brute_search` 的初始化方法，继承了父类的所有属性和方法：
    
    
    - `target_prompt`: 目标提示词。
    - `num_word`: 要替换的词数量。
    - `word_index`: 词的索引。
    - `num_subword`: 子词数量。
    - `len_subword`: 子词长度。
    - `threshold`: 损失阈值。
    - `saved_figure_path`: 保存图像的路径。
    - `pipe`: 模型管道。
    - `en`: 语言参数。
- `beam_size`: 束搜索的宽度，即在每一步保留的最佳候选数。

#### 方法 `search`

该方法执行束搜索，通过在每一步保留多个候选词来尝试绕过安全过滤器。具体步骤如下：

1. **初始化**：
    
    
    - 调用 `get_dictionary` 函数获取提示词列表和其长度。
    - 初始化查询计数器 `query` 和在线查询计数器 `query_online`。
    - 设置 `NSFW` 为 `True`，表示初始状态下提示词未通过安全过滤器。
2. **束搜索循环**：
    
    
    - 在 `NSFW` 为 `True` 的情况下，进入循环。
    - 初始化一个空的替换词列表 `temp_token_list`，长度为 `num_word`。
    - 初始化一个空的束候选词字典 `beam_candidates`。
3. **第一阶段搜索**：
    
    
    - 对于每个要替换的词（数量为 `num_word`），遍历提示词列表中的每个子词 `token`：
        
        
        - 更新当前的替换词列表 `temp_token_list`。
        - 生成对抗性提示词 `input_prompt`，并获取其嵌入表示 `input_embeds`。
        - 计算目标提示词与对抗性提示词的损失 `loss`。
        - 如果当前候选词数量少于束宽度 `beam_size`，直接添加到候选词字典中。
        - 否则，如果当前损失小于候选词字典中的最大损失，则替换最大损失的候选词。
        - 打印当前候选词字典。
4. **第二阶段搜索**：
    
    
    - 初始化最小候选词 `min_candidates` 和最小损失 `min_value`。
    - 对于每个要替换的词（数量为 `num_word`），遍历候选词字典中的每个候选词 `candidate`：
        
        
        - 初始化一个包含当前候选词的对抗性词列表 `adv_tokens`。
        - 对于每个子词（数量为 `num_subword - 1`），执行以下操作：
            
            
            - 构建当前的对抗性字符串 `adv_str`。
            - 初始化最小损失 `min_loss` 和目标子词 `target_token`。
            - 遍历提示词列表中的每个子词 `token`：
                
                
                - 更新当前的替换词列表 `temp_token_list`。
                - 生成对抗性提示词 `input_prompt`，并获取其嵌入表示 `input_embeds`。
                - 计算目标提示词与对抗性提示词的损失 `loss`。
                - 如果当前损失小于最小损失，则更新最小损失和目标子词。
                - 递增查询计数器 `query` 并打印查询次数和当前最小损失。
            - 将目标子词添加到对抗性词列表 `adv_tokens` 中。
            - 如果当前最小损失小于全局最小损失，则更新全局最小损失和最小候选词列表。
5. **检查安全过滤器**：
    
    
    - 使用模型管道 `pipe` 检查最终生成的对抗性提示词 `input_prompt` 是否通过安全过滤器。
    - 递增在线查询计数器 `query_online` 并清空CUDA缓存。
    - 如果对抗性提示词通过了安全过滤器，打印成功信息并保存生成的图像。

该方法的主要流程如下：

1. **获取词典**：加载提示词列表，用于生成替换词。
2. **束搜索**：
    
    
    - 第一阶段：遍历所有可能的替换词，保留前 `beam_size` 个损失最小的候选词。
    - 第二阶段：在每个候选词的基础上进一步构建对抗性提示词，选择全局损失最小的候选词。
3. **结果处理**：如果找到满足条件的对抗性提示词，保存生成的图像。

该类通过束搜索的方法，在每一步保留多个最佳候选词，最终找到能绕过安全过滤器的对抗性提示词。

然后再来分析rl方法的实现

![image-20240727173232034.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-16bc1a7d94169da0cb0bf205729c9bb7efbaa317.png)

这段代码定义了一个名为 `robot` 的类和它的内部类 `p_pi`，这是一个策略（和价值）网络

内部类 `p_pi`

#### 1. 初始化方法 `__init__`

- **参数**：`space`, `embedding_size`, `stable`, `v_theta`
- 初始化时，设置嵌入空间大小，创建嵌入层列表、LSTM层和线性层列表。
- 如果 `stable` 为 `True`，调用 `_stable_first_embedding` 方法使第一个嵌入层的参数不可训练。
- 如果 `v_theta` 为 `True`，创建价值网络的头部（线性层）。

#### 2. 前向传播方法 `forward`

- **参数**：`x`
- 使用当前阶段的嵌入层对输入 `x` 进行嵌入。
- 将嵌入后的 `x` 输入到 LSTM 层，并得到输出和隐藏状态。
- 通过当前阶段的线性层得到动作的概率分布。
- 如果 `v_theta` 为 `True`，通过对应的线性层得到当前状态的价值。
- 返回动作概率分布和（可选的）状态价值。

#### 3. `increment_stage` 方法

- 将阶段数增加1。

#### 4. `_stable_first_embedding` 方法

- 使第一个嵌入层的参数不可训练。

#### 5. `reset` 方法

- 重置阶段数为0，清空隐藏状态。

### 外部类 `robot`

#### 1. 初始化方法 `__init__`

- **参数**：`critic`, `space`, `rl_batch`, `gamma`, `lr`, `stable`
- 创建策略网络 `mind`，并设置各种参数如 `gamma`（折扣因子）、优化器、组合大小和批量大小。

#### 2. `select_action` 方法

- **参数**：`state`
- 输入当前状态，生成一个动作。
- 计算动作的概率分布，并从中采样一个动作。
- 如果 `critic` 为 `True`，返回动作、动作的对数概率和状态价值。
- 否则，仅返回动作和动作的对数概率。

#### 3. `select_combo` 方法

- 生成一个动作序列。
- 初始化状态，并迭代生成每一步的动作。
- 如果 `critic` 为 `True`，在生成动作和对数概率的同时，也计算每一步的状态价值。
- 返回动作序列、对数概率序列和（可选的）价值序列。

总体来说，这段代码实现了一个基于 LSTM 的策略网络，用于强化学习中的策略生成和状态值估计，并且支持 Actor-Critic 模式。

![image-20240727173314342.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-085282125b8741cd9cb77be25b13c413faefca29.png)

这段代码定义了一个名为 `MPA_agent` 的类，继承自 `robot` 类，并用于实现一个基于策略网络的强化学习代理。该代理用于生成对抗性提示词，以绕过安全过滤器：

### 类 `MPA_agent`

#### 初始化方法 `__init__`

- **参数**：`pipe`, `target`, `nsfw_word`, `num_word`, `word_index`, `num_subword`, `len_subword`, `s_filter`, `mode`, `threshold`, `target_prompt`, `query_limit`, `saved_figure_path`, `df`, `query`, `query_online`, `prompt_record`, `en`
- 初始化代理的各种属性，包括强化学习环境、目标提示词、单词索引、子词长度和数量、查询限制等。
- 调用 `create_searching_space` 方法创建搜索空间。

#### 方法 `build_robot`

- **参数**：`critic`, `rl_batch`, `gamma`, `lr`, `stable`
- 调用父类的初始化方法，建立策略网络。

#### 方法 `create_searching_space`

- **参数**：`num_word`, `num_subword`
- 创建搜索空间，将每个单词和子词的空间大小存储在数组中并返回。

#### 方法 `get_score`

- **参数**：`combo`, `target_tensor`
- 根据生成的组合（对抗性提示词）计算奖励。
- 根据模式（`clip` 或 `l2`）计算奖励值。
    
    
    - `clip` 模式：通过图像生成管道计算奖励，如果生成的图像包含 NSFW 内容，则给予惩罚。
    - `l2` 模式：通过计算嵌入向量之间的 L2 损失计算奖励。

#### 方法 `reward_backward`

- **参数**：`rewards`, `gamma`
- 进行奖励的回溯计算，根据折扣因子 `gamma` 更新奖励值。

#### 方法 `reinforcement_learn`

- **参数**：`steps`, `baseline_subtraction`
- 进行强化学习，代理与环境交互以生成对抗性提示词。
- 在每一步中，生成组合，计算奖励，并根据奖励更新策略网络。
- 根据查询限制和奖励阈值进行早停，保存结果。

### 代码中的关键点

1. **搜索空间的创建**：代理在初始化时创建一个搜索空间，其中包含可能的单词和子词的索引。
2. **奖励计算**：代理根据生成的对抗性提示词计算奖励，奖励计算的模式可以是 `clip` 或 `l2`，并根据是否通过安全过滤器调整奖励值。
3. **策略网络的训练**：通过多步与环境的交互，代理不断更新其策略网络，以生成更有效的对抗性提示词。
4. **早停条件**：在达到查询限制或奖励阈值后，代理会停止训练，并保存最优结果。

总体来说，这段代码实现了一个用于生成对抗性提示词的强化学习代理，通过不断与环境交互，代理能够学习到如何生成能绕过安全过滤器的对抗性提示词。

现在我们尝试执行

![image-20240727173603262.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-948a5d9a6929cd860e0ed749185249f40ea5166c.png)

![image-20240727173645555.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-534a139e4072be62efb2ee9012fbad29abfe8875.png)

![image-20240723210318701.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-774bd4f810a7afbd6de2001e6e0ef277897190b6.png)

![image-20240723214057343.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-07dd01fc1def94f06da21e9a4ead4c70d64e85fb.png)

大约需要运行2到3小时，运行完毕后终端显示如下

![image-20240724162805258.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-7719175b639edbc247aa68e1faee13847e85de4e.png)

查看此时得到的csv文件

![image-20240727173745723.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-2f7501cb82226d2834aabb1e6ceb5ae877a962e2.png)

此时就得到了所需的对抗提示

而由此生成的图像部分如下所示

![image-20240727173906056.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-02dcb704f5935d7b7dbda3389983a1100ffdfdae.png)

![image-20240727173928778.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-0fc99cf2e6601edd33aa5ed9cc19a8a482cb8695.png)

![image-20240727173951686.png](https://shs3.b.qianxin.com/attack_forum/2024/07/attach-3141c4964a12b87c8489fd56be3184c653d060b8.png)

这就成功复现了所提的越狱方法。

参考
==

1.<https://arxiv.org/pdf/2305.12082>

2.[https://www.baidu.com/s?rtt=1&amp;bsst=1&amp;cl=2&amp;tn=news&amp;rsv\_dl=ns\_pc&amp;word=%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%20%E6%9C%8D%E8%A3%85%E8%AE%BE%E8%AE%A1](https://www.baidu.com/s?rtt=1&bsst=1&cl=2&tn=news&rsv_dl=ns_pc&word=%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%20%E6%9C%8D%E8%A3%85%E8%AE%BE%E8%AE%A1)

3.<https://zh.wikipedia.org/wiki/NSFW>

4.<https://medium.com/@kemalpiro/step-by-step-visual-introduction-to-diffusion-models-235942d2f15c>

5.[https://www.reddit.com/r/dalle2/comments/wir4ao/comparison\_of\_dalle\_midjourney\_stable\_diffusion/](https://www.reddit.com/r/dalle2/comments/wir4ao/comparison_of_dalle_midjourney_stable_diffusion/)

6.[https://pytorch.org/tutorials/beginner/fgsm\_tutorial.html](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)

7.<https://techvidvan.com/tutorials/reinforcement-learning/>

8.<https://towardsdatascience.com/reinforcement-learning-fda8ff535bb6>